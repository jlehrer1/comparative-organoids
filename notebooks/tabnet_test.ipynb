{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "58f6ea9a",
   "metadata": {},
   "source": [
    "# TabNet Model Test\n",
    "\n",
    "In this notebook, we'll test a training loop for the TabNet model \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "eb9edaf4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['../data/interim/primary_bhaduri_T.csv',\n",
       "  '../data/interim/allen_cortex_T.csv',\n",
       "  '../data/interim/allen_m1_region_T.csv',\n",
       "  '../data/interim/whole_brain_bhaduri_T.csv'],\n",
       " ['../data/processed/labels/primary_bhaduri_labels.csv',\n",
       "  '../data/processed/labels/allen_cortex_labels.csv',\n",
       "  '../data/processed/labels/allen_m1_region_labels.csv',\n",
       "  '../data/processed/labels/whole_brain_bhaduri_labels.csv'])"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('../src')\n",
    "\n",
    "from models.lib.neural import *\n",
    "from models.lib.data import *\n",
    "from models.lib.train import *\n",
    "\n",
    "import helper \n",
    "from helper import gene_intersection\n",
    "from pytorch_tabnet.tab_network import TabNet\n",
    "\n",
    "import torch.nn as nn \n",
    "import torch.optim as optim\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "from torch.utils.data import Subset\n",
    "from helper import seed_everything\n",
    "\n",
    "seed_everything(42)\n",
    "t = helper.INTERIM_DATA_AND_LABEL_FILES_LIST\n",
    "datafiles, labelfiles = zip(*t.items())\n",
    "datafiles = [f'../data/interim/{f}' for f in datafiles]\n",
    "labelfiles = [f'../data/processed/labels/{f}' for f in labelfiles]\n",
    "refgenes = gene_intersection()\n",
    "\n",
    "datafiles, labelfiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "abd4e944",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytorch_lightning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7e117d9",
   "metadata": {},
   "source": [
    "First, we'll define our train, val and test sets, then generate the associated DataLoaders and try training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0c9b5ba7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<models.lib.data.CollateLoader at 0x7fbfee5c3cd0>,\n",
       " <models.lib.data.CollateLoader at 0x7fbfe995fe80>,\n",
       " <models.lib.data.CollateLoader at 0x7fbfee5a5250>,\n",
       " <models.lib.data.CollateLoader at 0x7fbfee5a1130>]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainloaders, valloaders, testloaders = generate_loaders(\n",
    "    datafiles=datafiles,\n",
    "    labelfiles=labelfiles,\n",
    "    class_label='Type',\n",
    "    refgenes=refgenes,\n",
    "    skip=3\n",
    ")\n",
    "\n",
    "trainloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0fd4b79d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<models.lib.data.CollateLoader at 0x7fbfee5c3cd0>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainloaders[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ba013226",
   "metadata": {},
   "outputs": [],
   "source": [
    "train, val, test = generate_single_dataset(\n",
    "    datafiles[0],\n",
    "    labelfiles[0],\n",
    "    'Type',\n",
    "    skip=3,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "25decdf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(len(train[0][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "de8462d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# trainloader_map, _, _ = generate_single_dataloader(\n",
    "#     datafile=datafiles[0], \n",
    "#     labelfile=labelfiles[0], \n",
    "#     class_label='Type',\n",
    "#     skip=3,\n",
    "#     map_genes=True\n",
    "# )\n",
    "\n",
    "# trainloader_nomap, _, _ = generate_single_dataloader(\n",
    "#     datafile=datafiles[0], \n",
    "#     labelfile=labelfiles[0], \n",
    "#     class_label='Type',\n",
    "#     skip=3,\n",
    "#     map_genes=False\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "050516bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i, (X, y) in enumerate(tqdm(trainloader_map)):\n",
    "#     if i == 200:\n",
    "#         break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5ee3b8ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i, (X, y) in enumerate(tqdm(trainloader_nomap)):\n",
    "#     if i == 200:\n",
    "#         break\n",
    "#     X = clean_sample(X, refgenes, train.features)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "79be48f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_loader, val_loader, test_loader = generate_loaders(\n",
    "#     datafiles,\n",
    "#     labelfiles,\n",
    "#     'Type',\n",
    "#     num_workers=0,\n",
    "#     collocate=True,\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f76bf878",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for X, y in train_loader:\n",
    "#     print(type(train_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9cb88c27",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16604"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(refgenes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f15f491c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.lib.neural import TabNetGeneClassifier\n",
    "\n",
    "model = TabNetGeneClassifier(\n",
    "    input_dim=len(refgenes),\n",
    "    output_dim=19\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d3775abc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SampleLoader(torch.utils.data.DataLoader):\n",
    "    def __init__(self, refgenes, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.refgenes = refgenes\n",
    "        self.currgenes = self.dataset.columns \n",
    "            \n",
    "    def __iter__(self):\n",
    "        for batch in super().__iter__():\n",
    "            yield clean_sample(batch[0], self.refgenes, self.currgenes), batch[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b1f6d065",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "         [0.9815, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "         [1.0537, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "         ...,\n",
       "         [0.7632, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "         [1.0228, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]]),\n",
       " tensor([ 6,  4,  4,  4, 16,  4,  4,  4, 16, 16,  4]))"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = SampleLoader(refgenes=refgenes, dataset=train, batch_size=11, num_workers=0)\n",
    "next(iter(test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fbc26eda",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([11, 16604])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(iter(test))[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7ff96570",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from functools import partial \n",
    "\n",
    "def custom_collate(sample):\n",
    "    data = torch.stack([x[0] for x in sample])\n",
    "    labels = torch.tensor([x[1] for x in sample])\n",
    "    return data, labels\n",
    "\n",
    "test = DataLoader(dataset=train, collate_fn=custom_collate, batch_size=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ac2fbb6",
   "metadata": {},
   "source": [
    "## PyTorch-Lightning compatible TabNet architecture "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6dd564eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.lib.neural import GeneClassifier\n",
    "from models.lib.neural import TabNetGeneClassifier\n",
    "import torch.functional as F\n",
    "\n",
    "base_model = TabNetGeneClassifier(\n",
    "    input_dim=len(refgenes),\n",
    "    output_dim=19,\n",
    ")\n",
    "\n",
    "class GeneClassifier(pl.LightningModule):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.base_model = TabNetGeneClassifier(\n",
    "            input_dim=input_dim,\n",
    "            output_dim=output_dim,\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # in lightning, forward defines the prediction/inference actions\n",
    "        out = self.base_model(x)\n",
    "        return out\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        # training_step defined the train loop.\n",
    "        # It is independent of forward\n",
    "        x, y = batch\n",
    "        y_hat = self(x)\n",
    "        \n",
    "        loss = F.cross_entropy(y_hat, y)\n",
    "        # Logging to TensorBoard by default\n",
    "        self.log(\"train_loss\", loss)\n",
    "        return loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=1e-3)\n",
    "        return optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "bd93830e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16604"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "base_model.input_dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "6a510881",
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = GeneClassifier(\n",
    "    input_dim=len(refgenes),\n",
    "    output_dim=19,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "466208c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.8448,  0.7586, -1.3998,  0.0651,  0.2534, -0.6008,  1.3020, -0.5995,\n",
       "         -1.2490, -1.7024,  0.7147, -0.1489, -0.7904,  1.3752, -0.3794, -0.2655,\n",
       "         -0.0343,  1.8146,  0.4887],\n",
       "        [-1.6459,  0.3161, -0.4659,  0.0587, -0.9774,  0.3582,  0.5182,  1.1889,\n",
       "         -0.3940,  0.8488,  0.5649,  0.2387,  0.6356,  1.2020,  1.1342, -0.2665,\n",
       "          1.1343,  1.5643, -0.0797],\n",
       "        [ 0.2180, -0.3618, -0.6595, -0.1235, -0.2284,  0.8975, -0.8446,  1.0732,\n",
       "          1.2151,  0.0133, -0.6522, -0.1236,  0.8981,  1.2028, -0.8417,  0.0313,\n",
       "         -0.6274,  1.7479, -0.7256],\n",
       "        [ 1.7444, -1.4065, -1.0772, -0.4756, -1.4739,  0.6613,  0.8600, -0.9243,\n",
       "          0.7876,  0.0924,  1.6400, -0.7856, -1.0382,  1.7035, -2.1024, -1.3298,\n",
       "          0.1758,  4.0664, -1.6334]], grad_fn=<MmBackward0>)"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample = next(iter(trainloaders[0]))[0]\n",
    "classifier(sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "9afa16de",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytorch_lightning as pl \n",
    "from typing import *\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from itertools import chain\n",
    "def custom_collate(sample, refgenes, currgenes):\n",
    "    data = clean_sample(torch.stack([x[0] for x in sample]), refgenes, currgenes)\n",
    "    labels = torch.tensor([x[1] for x in sample])\n",
    "    return data, labels\n",
    "\n",
    "class CollateLoader(torch.utils.data.DataLoader):\n",
    "    def __init__(self, refgenes, currgenes, *args, **kwargs):\n",
    "        collate_fn = functools.partial(custom_collate, refgenes=refgenes, currgenes=currgenes)\n",
    "        super().__init__(collate_fn = collate_fn, *args, **kwargs)\n",
    "\n",
    "class SequentialLoader:\n",
    "    def __init__(self, dataloaders):\n",
    "        self.dataloaders = dataloaders\n",
    "\n",
    "    def __len__(self):\n",
    "        return sum([len(dl) for dl in self.dataloaders])\n",
    "\n",
    "    def __iter__(self):\n",
    "        yield from chain(*self.dataloaders)\n",
    "                \n",
    "class GeneDataModule(pl.LightningDataModule):\n",
    "    def __init__(\n",
    "        self, \n",
    "        datafiles: List[str],\n",
    "        labelfiles: List[str],\n",
    "        class_label: str,\n",
    "        refgenes: List[str],\n",
    "        batch_size: int=16,\n",
    "        num_workers=32,\n",
    "        *args,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.__dict__.update(**kwargs)\n",
    "\n",
    "        self.datafiles = datafiles\n",
    "        self.labelfiles = labelfiles\n",
    "        self.class_label = class_label\n",
    "        self.refgenes = refgenes\n",
    "        \n",
    "        self.num_workers = num_workers\n",
    "        self.batch_size = batch_size\n",
    "        \n",
    "        self.trainloaders = []\n",
    "        self.valloaders = []\n",
    "        self.testloaders = []\n",
    "        \n",
    "        self.args = args\n",
    "        self.kwargs = kwargs\n",
    "        \n",
    "    def prepare_data(self):\n",
    "        # Download data from S3 here \n",
    "        pass \n",
    "    \n",
    "    def setup(self, stage: Optional[str] = None):\n",
    "        trainloaders, valloaders, testloaders = generate_loaders(\n",
    "            datafiles=self.datafiles,\n",
    "            labelfiles=self.labelfiles,\n",
    "            class_label=self.class_label,\n",
    "            refgenes=self.refgenes,\n",
    "            batch_size=self.batch_size,\n",
    "            num_workers=self.num_workers,\n",
    "            *self.args,\n",
    "            **self.kwargs\n",
    "        )\n",
    "        \n",
    "        self.trainloaders = SequentialLoader(trainloaders)\n",
    "        self.valloaders = SequentialLoader(valloaders)\n",
    "        self.testloaders = SequentialLoader(testloaders)\n",
    "        \n",
    "    def train_dataloader(self):\n",
    "        return self.trainloaders\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return self.valloaders\n",
    "\n",
    "    def test_dataloader(self):\n",
    "        return self.testloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "7429cce3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[1.1458, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "         [0.8902, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]]),\n",
       " tensor([ 6,  8, 17,  4]))"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s = SequentialLoader(trainloaders)\n",
    "next(iter(s))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "d1e4cfa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "module = GeneDataModule(\n",
    "    datafiles=datafiles, \n",
    "    labelfiles=labelfiles, \n",
    "    class_label='Type', \n",
    "    refgenes=refgenes,\n",
    "    skip=3, \n",
    "    normalize=True,\n",
    "    batch_size=8,\n",
    "    num_workers=0,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9fea2b9",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "/Users/julian/miniconda3/envs/base-data-science/lib/python3.9/site-packages/pytorch_lightning/trainer/configuration_validator.py:116: UserWarning: You passed in a `val_dataloader` but have no `validation_step`. Skipping val loop.\n",
      "  rank_zero_warn(\"You passed in a `val_dataloader` but have no `validation_step`. Skipping val loop.\")\n",
      "/Users/julian/miniconda3/envs/base-data-science/lib/python3.9/site-packages/pytorch_lightning/loggers/wandb.py:341: UserWarning: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.\n",
      "  rank_zero_warn(\n",
      "\n",
      "  | Name       | Type                 | Params\n",
      "----------------------------------------------------\n",
      "0 | base_model | TabNetGeneClassifier | 1.1 M \n",
      "----------------------------------------------------\n",
      "1.1 M     Trainable params\n",
      "0         Non-trainable params\n",
      "1.1 M     Total params\n",
      "4.407     Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6a5844d8703f4ec79e585bff328d5711",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pytorch_lightning import Trainer\n",
    "from pytorch_lightning.loggers import WandbLogger\n",
    "\n",
    "wandb_logger = WandbLogger(name='TabNet Gene Classifier')\n",
    "trainer = Trainer(logger=wandb_logger)\n",
    "trainer.fit(classifier, datamodule=module)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c2124a9",
   "metadata": {},
   "source": [
    "Now, we'll subset and define our DataLoaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31975f3d",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import wandb\n",
    "from torchmetrics.functional import accuracy\n",
    "\n",
    "wandb.init()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\n",
    "\n",
    "train_loss = []\n",
    "val_loss = []\n",
    "test_loss = []\n",
    "\n",
    "mod = 10\n",
    "wandb.watch(model)\n",
    "for epoch in range(1):  # loop over the dataset multiple times\n",
    "    running_loss = 0.0\n",
    "    epoch_loss = 0.0\n",
    "    # Train loop\n",
    "    model.train()\n",
    "    for idx, train in enumerate(train_loader):\n",
    "        print(f'On loader {idx = }')\n",
    "        for i, data in enumerate(tqdm(train)):\n",
    "            print(f'On minibatch {i = }/10')\n",
    "            if i == 10:\n",
    "                break \n",
    "            inputs, labels = data\n",
    "            # CLEAN INPUTS\n",
    "            inputs = clean_sample(inputs, refgenes, train.dataset.columns)\n",
    "            # Forward pass ➡\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            # Backward pass ⬅\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "\n",
    "            # Step with optimizer\n",
    "            optimizer.step()\n",
    "\n",
    "            # print statistics\n",
    "            running_loss += loss.item()\n",
    "            epoch_loss += loss.item()\n",
    "\n",
    "\n",
    "            if i % mod == 0: # record every 2000 mini batches \n",
    "                metric_results = calculate_metrics(\n",
    "                    outputs=outputs,\n",
    "                    labels=labels,\n",
    "                    append_str='train',\n",
    "                    num_classes=model.output_dim,\n",
    "                    subset='weighted_accuracy',\n",
    "                )\n",
    "\n",
    "                wandb.log(metric_results)\n",
    "                running_loss = running_loss / mod\n",
    "                wandb.log({f\"batch_train_loss\": loss})\n",
    "\n",
    "                running_loss = 0.0\n",
    "            \n",
    "    wandb.log({f\"epoch_train_loss\": epoch_loss / len(train)})\n",
    "    \n",
    "    model.eval()\n",
    "    with torch.no_grad(): # save memory but not computing gradients \n",
    "        running_loss = 0.0\n",
    "        epoch_loss = 0.0\n",
    "        \n",
    "        for val in val_loader:\n",
    "            print(f'On loader {i = }')\n",
    "            for i, data in enumerate(val):\n",
    "                if i == 10:\n",
    "                    break \n",
    "                inputs, labels = data\n",
    "                # CLEAN INPUTS\n",
    "                inputs = clean_sample(inputs, refgenes, val.dataset.columns)\n",
    "                # Forward pass ➡\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "\n",
    "                # print statistics\n",
    "                running_loss += loss.item()\n",
    "                epoch_loss += loss.item()\n",
    "\n",
    "                if i % mod == 0: #every 2000 mini batches \n",
    "                    running_loss = running_loss / mod\n",
    "                    wandb.log({\"val_loss\": loss})\n",
    "                    running_loss = 0.0\n",
    "\n",
    "                    metric_results = calculate_metrics(\n",
    "                        outputs=outputs,\n",
    "                        labels=labels,\n",
    "                        num_classes=model.output_dim,\n",
    "                        subset='weighted_accuracy',\n",
    "                        append_str='val',\n",
    "                    )\n",
    "\n",
    "                wandb.log(metric_results)\n",
    "    \n",
    "        wandb.log({f\"epoch_val_loss\": epoch_loss / len(train)})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50bdb29c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fefd4305",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def test_loop(\n",
    "    model,\n",
    "    testloaders,\n",
    "    refgenes,\n",
    "    criterion,\n",
    "    mod,\n",
    "):\n",
    "    model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for idx, test in enumerate(testloaders):\n",
    "            print(f'On {idx = }')\n",
    "            running_loss = 0.0\n",
    "            for i, data in enumerate(test):\n",
    "                print(f'minibatch {i = }')\n",
    "                if i == 10:\n",
    "                    break\n",
    "                inputs, labels = data\n",
    "                # CLEAN INPUTS\n",
    "                inputs = clean_sample(inputs, refgenes, test.dataset.columns)\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "\n",
    "                # print statistics\n",
    "                running_loss += loss.item()\n",
    "                if i % mod == 0: #every 2000 mini batches \n",
    "                    running_loss = running_loss / mod\n",
    "                    wandb.log({\"test_loss\": loss})\n",
    "                    running_loss = 0.0\n",
    "\n",
    "                    metric_results = calculate_metrics(\n",
    "                        outputs=outputs,\n",
    "                        labels=labels,\n",
    "                        num_classes=model.output_dim,\n",
    "                        subset='weighted_accuracy',\n",
    "                        append_str='test',\n",
    "                    )\n",
    "\n",
    "                    wandb.log(metric_results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "955f0944",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "test_loop(model, test_loader, refgenes, criterion, mod)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7666179",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt \n",
    "\n",
    "plt.plot(train_loss, label='Train')\n",
    "plt.plot(val_loss, label='Val')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dab9b869",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aabc85c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "labelfiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c1a6fc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import memmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2099528",
   "metadata": {},
   "outputs": [],
   "source": [
    "f = memmap('../data/interim/allen_cortex_T.csv', dtype=np.float64, mode='r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9376629c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base-data-science]",
   "language": "python",
   "name": "conda-env-base-data-science-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
