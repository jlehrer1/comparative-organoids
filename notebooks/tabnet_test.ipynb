{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5e1292dc",
   "metadata": {},
   "source": [
    "# TabNet Model Test\n",
    "\n",
    "In this notebook, we'll test a training loop for the TabNet model \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "11510c84",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../src')\n",
    "\n",
    "from models.lib.neural import *\n",
    "from models.lib.data import *\n",
    "from models.lib.train import *\n",
    "\n",
    "import helper \n",
    "from helper import gene_intersection\n",
    "from pytorch_tabnet.tab_network import TabNet\n",
    "\n",
    "import torch.nn as nn \n",
    "import torch.optim as optim\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "from torch.utils.data import Subset\n",
    "from helper import seed_everything\n",
    "\n",
    "seed_everything(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9813ede7",
   "metadata": {},
   "source": [
    "First, we'll define our train, val and test sets, then generate the associated DataLoaders and try training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1d8d9744",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['../data/interim/primary_bhaduri_T.csv',\n",
       "  '../data/interim/allen_cortex_T.csv',\n",
       "  '../data/interim/allen_m1_region_T.csv',\n",
       "  '../data/interim/whole_brain_bhaduri_T.csv'],\n",
       " ['../data/processed/labels/primary_bhaduri_labels.csv',\n",
       "  '../data/processed/labels/allen_cortex_labels.csv',\n",
       "  '../data/processed/labels/allen_m1_region_labels.csv',\n",
       "  '../data/processed/labels/whole_brain_bhaduri_labels.csv'])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t = helper.INTERIM_DATA_AND_LABEL_FILES_LIST\n",
    "datafiles, labelfiles = zip(*t.items())\n",
    "datafiles = [f'../data/interim/{f}' for f in datafiles]\n",
    "labelfiles = [f'../data/processed/labels/{f}' for f in labelfiles]\n",
    "\n",
    "datafiles, labelfiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fe7d1797",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader, val_loader, test_loader = generate_loaders(\n",
    "    datafiles,\n",
    "    labelfiles,\n",
    "    'Type',\n",
    "    num_workers=0,\n",
    "    collocate=False,\n",
    ")\n",
    "\n",
    "refgenes = gene_intersection()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b781e3de",
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.lib.neural import TabNetGeneClassifier\n",
    "\n",
    "model = TabNetGeneClassifier(\n",
    "    input_dim=len(refgenes),\n",
    "    output_dim=18\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3c94ecec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "        [0.9815, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "        [1.0537, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "        [1.0578, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample = next(iter(train_loader[0]))[0]\n",
    "sample = clean_sample(sample, refgenes, train_loader[0].dataset.columns)\n",
    "sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5ee4f121",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.2157, -0.5378,  0.0325, -0.4296, -0.6017,  1.8330,  0.3227, -0.2993,\n",
       "          0.3952,  2.5684,  3.7755,  1.5207,  2.0239, -1.0538,  0.9257,  2.2742,\n",
       "         -1.4293,  0.0628],\n",
       "        [ 2.0681,  0.3052,  0.7337, -0.6232, -2.1632,  2.6200,  2.9574, -3.0004,\n",
       "         -1.6573,  0.9772,  0.2851, -1.0471,  2.5345, -0.0687,  0.4001,  3.0233,\n",
       "         -0.8894, -0.3621],\n",
       "        [ 0.5813,  0.6007, -0.1726,  0.3551, -1.4737,  2.1677,  2.3966, -1.5421,\n",
       "         -1.9907,  0.8255,  1.7362,  1.2050,  0.6144, -0.8192, -1.0613,  1.9794,\n",
       "         -0.6014, -0.9850],\n",
       "        [ 1.9329,  0.8326,  0.8148, -0.3646, -1.7832,  2.4768,  2.5341, -2.3260,\n",
       "         -1.3227,  1.2317,  1.5714, -1.0348,  1.5763, -1.4306,  0.0840,  2.3380,\n",
       "         -0.9355, -0.3554]], grad_fn=<MmBackward0>)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46950c18",
   "metadata": {},
   "source": [
    "Now, we'll subset and define our DataLoaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6faf1df0",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:1oznd6n7) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced <strong style=\"color:#cdcd00\">earthy-blaze-31</strong>: <a href=\"https://wandb.ai/jlehrer1/organoid-classification-notebooks/runs/1oznd6n7\" target=\"_blank\">https://wandb.ai/jlehrer1/organoid-classification-notebooks/runs/1oznd6n7</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20220403_180006-1oznd6n7/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:1oznd6n7). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.12.11"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/Users/julian/Documents/Projects/organoid-classification/notebooks/wandb/run-20220403_180221-1272fhpj</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/jlehrer1/organoid-classification-notebooks/runs/1272fhpj\" target=\"_blank\">distinctive-disco-32</a></strong> to <a href=\"https://wandb.ai/jlehrer1/organoid-classification-notebooks\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "TypeError",
     "evalue": "cross_entropy_loss(): argument 'input' (position 1) must be Tensor, not tuple",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/pd/jsjcl0fn7w57s5mfr34b20pm0000gn/T/ipykernel_97147/1507193741.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     24\u001b[0m             \u001b[0;31m# Forward pass ➡\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m             \u001b[0;31m# Backward pass ⬅\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/base-data-science/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/base-data-science/lib/python3.9/site-packages/torch/nn/modules/loss.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, target)\u001b[0m\n\u001b[1;32m   1148\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1149\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1150\u001b[0;31m         return F.cross_entropy(input, target, weight=self.weight,\n\u001b[0m\u001b[1;32m   1151\u001b[0m                                \u001b[0mignore_index\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mignore_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduction\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreduction\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1152\u001b[0m                                label_smoothing=self.label_smoothing)\n",
      "\u001b[0;32m~/miniconda3/envs/base-data-science/lib/python3.9/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mcross_entropy\u001b[0;34m(input, target, weight, size_average, ignore_index, reduce, reduction, label_smoothing)\u001b[0m\n\u001b[1;32m   2844\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0msize_average\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mreduce\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2845\u001b[0m         \u001b[0mreduction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegacy_get_string\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msize_average\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduce\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2846\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_nn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcross_entropy_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_enum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreduction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel_smoothing\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2847\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2848\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: cross_entropy_loss(): argument 'input' (position 1) must be Tensor, not tuple"
     ]
    }
   ],
   "source": [
    "import wandb\n",
    "from torchmetrics.functional import accuracy\n",
    "\n",
    "wandb.init()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\n",
    "\n",
    "train_loss = []\n",
    "val_loss = []\n",
    "test_loss = []\n",
    "\n",
    "mod = 10\n",
    "wandb.watch(model)\n",
    "for epoch in range(1000):  # loop over the dataset multiple times\n",
    "    running_loss = 0.0\n",
    "    epoch_loss = 0.0\n",
    "    # Train loop\n",
    "    model.train()\n",
    "    for train in train_loader:\n",
    "        for i, data in enumerate(train):\n",
    "            inputs, labels = data\n",
    "            # CLEAN INPUTS\n",
    "            inputs = clean_sample(inputs, refgenes, train.dataset.columns)\n",
    "            # Forward pass ➡\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            # Backward pass ⬅\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "\n",
    "            # Step with optimizer\n",
    "            optimizer.step()\n",
    "\n",
    "            # print statistics\n",
    "            running_loss += loss.item()\n",
    "            epoch_loss += loss.item()\n",
    "\n",
    "\n",
    "            if i % mod == 0: # record every 2000 mini batches \n",
    "                metric_results = calculate_metrics(\n",
    "                    outputs=outputs,\n",
    "                    labels=labels,\n",
    "                    append_str='train',\n",
    "                    num_classes=model.output_dim,\n",
    "                    subset='weighted_accuracy',\n",
    "                )\n",
    "\n",
    "                wandb.log(metric_results)\n",
    "                running_loss = running_loss / mod\n",
    "                wandb.log({f\"batch_train_loss\": loss})\n",
    "\n",
    "                running_loss = 0.0\n",
    "            \n",
    "    wandb.log({f\"epoch_train_loss\": epoch_loss / len(train)})\n",
    "    \n",
    "    model.eval()\n",
    "    with torch.no_grad(): # save memory but not computing gradients \n",
    "        running_loss = 0.0\n",
    "        epoch_loss = 0.0\n",
    "        \n",
    "        for val in val_loader:\n",
    "            for i, data in enumerate(val):\n",
    "                inputs, labels = data\n",
    "                # CLEAN INPUTS\n",
    "                inputs = clean_sample(inputs, refgenes, val.dataset.columns)\n",
    "                # Forward pass ➡\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "\n",
    "                # print statistics\n",
    "                running_loss += loss.item()\n",
    "                epoch_loss += loss.item()\n",
    "\n",
    "                if i % mod == 0: #every 2000 mini batches \n",
    "                    running_loss = running_loss / mod\n",
    "                    wandb.log({\"val_loss\": loss})\n",
    "                    running_loss = 0.0\n",
    "\n",
    "                    metric_results = calculate_metrics(\n",
    "                        outputs=outputs,\n",
    "                        labels=labels,\n",
    "                        num_classes=model.output_dim,\n",
    "                        subset='weighted_accuracy',\n",
    "                        append_str='val',\n",
    "                    )\n",
    "\n",
    "                wandb.log(metric_results)\n",
    "    \n",
    "        wandb.log({f\"epoch_val_loss\": epoch_loss / len(train)})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b379943",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt \n",
    "\n",
    "plt.plot(train_loss, label='Train')\n",
    "plt.plot(val_loss, label='Val')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bfafb8b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "556093c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "subset = ['asdf']\n",
    "subset = ([subset] if isinstance(subset, str) else subset)\n",
    "subset"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base-data-science]",
   "language": "python",
   "name": "conda-env-base-data-science-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
