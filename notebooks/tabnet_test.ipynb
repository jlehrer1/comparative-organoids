{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "58f6ea9a",
   "metadata": {},
   "source": [
    "# TabNet Model Test\n",
    "\n",
    "In this notebook, we'll test a training loop for the TabNet model \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "eb9edaf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../src')\n",
    "\n",
    "from models.lib.neural import *\n",
    "from models.lib.data import *\n",
    "from models.lib.train import *\n",
    "\n",
    "import helper \n",
    "from helper import gene_intersection\n",
    "from pytorch_tabnet.tab_network import TabNet\n",
    "\n",
    "import torch.nn as nn \n",
    "import torch.optim as optim\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "from torch.utils.data import Subset\n",
    "import matplotlib.pyplot as plt \n",
    "from models.lib.lightning_train import *\n",
    "\n",
    "t = helper.INTERIM_DATA_AND_LABEL_FILES_LIST\n",
    "datafiles, labelfiles = zip(*t.items())\n",
    "datafiles = [f'../data/interim/{f}' for f in datafiles]\n",
    "labelfiles = [f'../data/processed/labels/{f}' for f in labelfiles]\n",
    "refgenes = gene_intersection()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "66657fab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Args passed to DataLoader init are dict_keys(['batch_size'])\n",
      "Args passed to DataLoader init are dict_keys(['batch_size'])\n",
      "Args passed to DataLoader init are dict_keys(['batch_size'])\n"
     ]
    }
   ],
   "source": [
    "train, val, test = generate_single_dataloader(\n",
    "    datafile=datafiles[0], \n",
    "    labelfile=labelfiles[0], \n",
    "    class_label='Type', \n",
    "    skip=3,\n",
    "    batch_size=4,\n",
    "    transpose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cb1a4aa5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device is cpu\n",
      "../data/interim/primary_bhaduri_T.csv exists, continuing...\n",
      "../data/processed/labels/primary_bhaduri_labels.csv exists, continuing...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model initialized. input_dim = 16604, output_dim = 19. Metrics are dict_keys(['accuracy', 'precision', 'recall']) and weighted_metrics = False\n"
     ]
    }
   ],
   "source": [
    "trainer, model, module = generate_trainer(\n",
    "    datafiles[0:1], \n",
    "    labelfiles[0:1], \n",
    "    class_label='Type', \n",
    "    skip=3, \n",
    "    drop_last=True,\n",
    "    batch_size=2,\n",
    "    num_workers=0,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3216e760",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating dataloaders...\n",
      "Args passed to DataLoader init are dict_keys(['batch_size', 'shuffle', 'num_workers', 'drop_last'])\n",
      "Args passed to DataLoader init are dict_keys(['batch_size', 'shuffle', 'num_workers', 'drop_last'])\n",
      "Args passed to DataLoader init are dict_keys(['batch_size', 'shuffle', 'num_workers', 'drop_last'])\n",
      "Done, continuing to training.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mjlehrer1\u001b[0m (use `wandb login --relogin` to force relogin)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.12.14 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.12.11"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/Users/julian/Documents/Projects/organoid-classification/notebooks/wandb/run-20220409_155645-32k2c7u3</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/jlehrer1/cell-classifier-Type/runs/32k2c7u3\" target=\"_blank\">TabNet Classifier, Shuffle=True</a></strong> to <a href=\"https://wandb.ai/jlehrer1/cell-classifier-Type\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  | Name       | Type                 | Params\n",
      "----------------------------------------------------\n",
      "0 | base_model | TabNetGeneClassifier | 1.1 M \n",
      "----------------------------------------------------\n",
      "1.1 M     Trainable params\n",
      "0         Non-trainable params\n",
      "1.1 M     Total params\n",
      "4.407     Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation sanity check: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/julian/miniconda3/envs/base-data-science/lib/python3.9/site-packages/pytorch_lightning/trainer/data_loading.py:111: UserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 10 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "/Users/julian/miniconda3/envs/base-data-science/lib/python3.9/site-packages/pytorch_lightning/trainer/data_loading.py:111: UserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 10 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "32bea220c5c04257bc3a007e4e21be6b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainer.fit(model, datamodule=module)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7e117d9",
   "metadata": {},
   "source": [
    "First, we'll define our train, val and test sets, then generate the associated DataLoaders and try training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f15f491c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0903, 0.5162, 0.5914, 0.3951, 0.4686],\n",
       "        [0.0369, 0.3091, 0.5891, 0.3497, 0.6587],\n",
       "        [0.4281, 0.4271, 0.3070, 0.1967, 0.7081],\n",
       "        [0.0412, 0.0270, 0.6903, 0.6909, 0.2090],\n",
       "        [0.7512, 0.4057, 0.1915, 0.4484, 0.1827],\n",
       "        [0.0441, 0.6361, 0.5088, 0.0429, 0.5769],\n",
       "        [0.5126, 0.6286, 0.2299, 0.0936, 0.5296],\n",
       "        [0.8367, 0.3219, 0.4290, 0.0271, 0.1074],\n",
       "        [0.2886, 0.5081, 0.1301, 0.5413, 0.5905],\n",
       "        [0.4303, 0.2994, 0.5208, 0.5857, 0.3330]], dtype=torch.float64)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "F.normalize(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ac2fbb6",
   "metadata": {},
   "source": [
    "## PyTorch-Lightning compatible TabNet architecture "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6dd564eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.lib.neural import TabNetGeneClassifier\n",
    "import torch.functional as F\n",
    "from torchmetrics.functional import accuracy, precision, recall \n",
    "\n",
    "base_model = TabNetGeneClassifier(\n",
    "    input_dim=len(refgenes),\n",
    "    output_dim=19,\n",
    ")\n",
    "\n",
    "class GeneClassifier(pl.LightningModule):\n",
    "    \"\"\"\n",
    "        Initialize the gene classifier neural network\n",
    "\n",
    "        Parameters:\n",
    "        input_dim: Number of features in the inpute matrix \n",
    "        output_dim: Number of classes\n",
    "        weights: Weights to use in loss calculation to account for imbalance in class size \n",
    "        params: Dictionary of hyperparameters to use. Must include width, layers, lr, momentum, weight_decay\n",
    "        metrics: Dictionary of metrics to log, where keys are metric names and values are torchmetrics.functional methods\n",
    "        weighted_metrics: If True, use class-weighted calculation in metrics. Otherwise, use default 'micro' calculation.\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self, \n",
    "        input_dim, \n",
    "        output_dim,\n",
    "        base_model=None,\n",
    "        optimizer=torch.optim.Adam,\n",
    "        optim_params: Dict[str, float]={\n",
    "            'lr': 0.001,\n",
    "            'weight_decay': 0.01,\n",
    "        },\n",
    "        metrics: Dict[str, Callable]={\n",
    "            'accuracy': accuracy,\n",
    "            'precision': precision,\n",
    "            'recall': recall,\n",
    "        },\n",
    "        weighted_metrics=False,\n",
    "        *args,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        print(f'Model initialized. {input_dim = }, {output_dim = }. Metrics are {metrics.keys()} and {weighted_metrics = }')\n",
    "\n",
    "        if base_model is None:\n",
    "            self.base_model = TabNetGeneClassifier(\n",
    "                input_dim=input_dim,\n",
    "                output_dim=output_dim,\n",
    "                *args,\n",
    "                **kwargs,\n",
    "            )\n",
    "        else:\n",
    "            self.base_model = base_model\n",
    "        \n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = output_dim\n",
    "        self.optimizer = optimizer\n",
    "        self.optim_params = optim_params\n",
    "        self.metrics = metrics\n",
    "        self.weighted_metrics = weighted_metrics\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = self.base_model(x)\n",
    "        return out\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        y_hat = self(x)\n",
    "        \n",
    "        loss = F.cross_entropy(y_hat, y)\n",
    "        \n",
    "        self.log(\"train_loss\", loss, logger=True, on_epoch=True, on_step=True)\n",
    "        self._compute_metrics(y_hat, y, 'train')\n",
    "        return loss\n",
    "    \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        y_hat = self(x)\n",
    "        val_loss = F.cross_entropy(y_hat, y)\n",
    "\n",
    "        self.log(\"val_loss\", val_loss, logger=True, on_epoch=True, on_step=True)\n",
    "        self._compute_metrics(y_hat, y, 'val')\n",
    "        \n",
    "        return val_loss\n",
    "    \n",
    "    def _compute_metrics(self, y_hat, y, tag, on_epoch=True, on_step=True):\n",
    "        for name, metric in self.metrics.items():\n",
    "            if not self.weighted_metrics: # We dont consider class support in calculation\n",
    "                val = metric(y_hat, y, average='weighted', num_classes=self.output_dim)\n",
    "                self.log(\n",
    "                    f\"weighted_{tag}_{name}\", \n",
    "                    val, \n",
    "                    on_epoch=on_epoch, \n",
    "                    on_step=on_step,\n",
    "                    logger=True,\n",
    "                )\n",
    "            else:\n",
    "                val = metric(y_hat, y)\n",
    "                self.log(\n",
    "                    f\"{tag}_{name}\", \n",
    "                    val, \n",
    "                    on_epoch=on_epoch, \n",
    "                    on_step=on_step,\n",
    "                    logger=True,\n",
    "                )\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = self.optimizer(self.parameters(), **self.optim_params)\n",
    "        return optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e34934de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16604"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "base_model.input_dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6a510881",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model initialized. input_dim = 16604, output_dim = 19. Metrics are dict_keys(['accuracy', 'precision', 'recall']) and weighted_metrics = False\n"
     ]
    }
   ],
   "source": [
    "classifier = GeneClassifier(\n",
    "    input_dim=len(refgenes),\n",
    "    output_dim=19,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2dfac175",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytorch_lightning as pl \n",
    "from typing import *\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from itertools import chain\n",
    "\n",
    "def custom_collate(sample, refgenes, currgenes):\n",
    "    data = clean_sample(torch.stack([x[0] for x in sample]), refgenes, currgenes)\n",
    "    labels = torch.tensor([x[1] for x in sample])\n",
    "    return data, labels\n",
    "\n",
    "class CollateLoader(torch.utils.data.DataLoader):\n",
    "    def __init__(self, refgenes, currgenes, *args, **kwargs):\n",
    "        collate_fn = functools.partial(custom_collate, refgenes=refgenes, currgenes=currgenes)\n",
    "        super().__init__(collate_fn = collate_fn, *args, **kwargs)\n",
    "\n",
    "class SequentialLoader:\n",
    "    def __init__(self, dataloaders):\n",
    "        self.dataloaders = dataloaders\n",
    "\n",
    "    def __len__(self):\n",
    "        return sum([len(dl) for dl in self.dataloaders])\n",
    "\n",
    "    def __iter__(self):\n",
    "        yield from chain(*self.dataloaders)\n",
    "                \n",
    "class GeneDataModule(pl.LightningDataModule):\n",
    "    def __init__(\n",
    "        self, \n",
    "        datafiles: List[str],\n",
    "        labelfiles: List[str],\n",
    "        class_label: str,\n",
    "        refgenes: List[str],\n",
    "        batch_size: int=16,\n",
    "        num_workers=32,\n",
    "        shuffle=False,\n",
    "        *args,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.__dict__.update(**kwargs)\n",
    "\n",
    "        self.datafiles = datafiles\n",
    "        self.labelfiles = labelfiles\n",
    "        self.class_label = class_label\n",
    "        self.refgenes = refgenes\n",
    "        self.shuffle = shuffle\n",
    "        \n",
    "        self.num_workers = num_workers\n",
    "        self.batch_size = batch_size\n",
    "        \n",
    "        self.trainloaders = []\n",
    "        self.valloaders = []\n",
    "        self.testloaders = []\n",
    "        \n",
    "        self.args = args\n",
    "        self.kwargs = kwargs             \n",
    "\n",
    "    def setup(self, stage: Optional[str] = None):\n",
    "        print('Creating dataloaders...')\n",
    "        trainloaders, valloaders, testloaders = generate_loaders(\n",
    "            datafiles=self.datafiles,\n",
    "            labelfiles=self.labelfiles,\n",
    "            class_label=self.class_label,\n",
    "            refgenes=self.refgenes,\n",
    "            batch_size=self.batch_size,\n",
    "            num_workers=self.num_workers,\n",
    "            shuffle=self.shuffle,\n",
    "            collocate=True, # Join all loaders into one sequential one \n",
    "            *self.args,\n",
    "            **self.kwargs\n",
    "        )\n",
    "        \n",
    "        print('Done, continuing to training.')\n",
    "\n",
    "        self.trainloaders = trainloaders\n",
    "        self.valloaders = valloaders\n",
    "        self.testloaders = testloaders\n",
    "        \n",
    "    def train_dataloader(self):\n",
    "        return self.trainloaders\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return self.valloaders\n",
    "\n",
    "    def test_dataloader(self):\n",
    "        return self.testloaders\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e824c545",
   "metadata": {},
   "outputs": [],
   "source": [
    "module = GeneDataModule(\n",
    "    datafiles=datafiles, \n",
    "    labelfiles=labelfiles, \n",
    "    class_label='Type',\n",
    "    refgenes=refgenes,\n",
    "    skip=3,\n",
    "    normalize=True,\n",
    "    batch_size=8,\n",
    "    num_workers=0,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63cc997b",
   "metadata": {},
   "outputs": [],
   "source": [
    "train, val, test = generate_loaders(datafiles=datafiles, labelfiles=labelfiles, class_label='Type', refgenes=refgenes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3ff363a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d50005bd",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating dataloaders...\n"
     ]
    }
   ],
   "source": [
    "from pytorch_lightning import Trainer \n",
    "from pytorch_lightning.loggers import WandbLogger\n",
    "\n",
    "wandb_logger = WandbLogger(project='TabNet Cell Type Classifier', name='Tabnet with Metrics')\n",
    "trainer = Trainer(logger=wandb_logger)\n",
    "\n",
    "trainer.fit(classifier, datamodule=module)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e1ec01a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.lib.lightning_train import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b8bb665",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer, model, module = generate_trainer(\n",
    "    \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f963c6cf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21eefd27",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84e2d9eb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6fc9356",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "941c35ca",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "prepare_data('../data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f4ba28c",
   "metadata": {},
   "outputs": [],
   "source": [
    "t.keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f1f5799",
   "metadata": {},
   "outputs": [],
   "source": [
    "t.values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbee6c85",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base-data-science] *",
   "language": "python",
   "name": "conda-env-base-data-science-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
