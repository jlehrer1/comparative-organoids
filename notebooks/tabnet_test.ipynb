{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "58f6ea9a",
   "metadata": {},
   "source": [
    "# TabNet Model Test\n",
    "\n",
    "In this notebook, we'll test a training loop for the TabNet model \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "eb9edaf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../src')\n",
    "\n",
    "from models.lib.neural import *\n",
    "from models.lib.data import *\n",
    "from models.lib.train import *\n",
    "\n",
    "import helper \n",
    "from helper import gene_intersection\n",
    "from pytorch_tabnet.tab_network import TabNet\n",
    "\n",
    "import torch.nn as nn \n",
    "import torch.optim as optim\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "from torch.utils.data import Subset\n",
    "from helper import seed_everything\n",
    "\n",
    "seed_everything(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7e117d9",
   "metadata": {},
   "source": [
    "First, we'll define our train, val and test sets, then generate the associated DataLoaders and try training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "113c0f7f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['../data/interim/primary_bhaduri_T.csv',\n",
       "  '../data/interim/allen_cortex_T.csv',\n",
       "  '../data/interim/allen_m1_region_T.csv',\n",
       "  '../data/interim/whole_brain_bhaduri_T.csv'],\n",
       " ['../data/processed/labels/primary_bhaduri_labels.csv',\n",
       "  '../data/processed/labels/allen_cortex_labels.csv',\n",
       "  '../data/processed/labels/allen_m1_region_labels.csv',\n",
       "  '../data/processed/labels/whole_brain_bhaduri_labels.csv'])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t = helper.INTERIM_DATA_AND_LABEL_FILES_LIST\n",
    "datafiles, labelfiles = zip(*t.items())\n",
    "datafiles = [f'../data/interim/{f}' for f in datafiles]\n",
    "labelfiles = [f'../data/processed/labels/{f}' for f in labelfiles]\n",
    "\n",
    "datafiles, labelfiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "79be48f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader, val_loader, test_loader = generate_loaders(\n",
    "    datafiles,\n",
    "    labelfiles,\n",
    "    'Type',\n",
    "    num_workers=0,\n",
    "    collocate=False,\n",
    ")\n",
    "\n",
    "refgenes = gene_intersection()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f15f491c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.lib.neural import TabNetGeneClassifier\n",
    "\n",
    "model = TabNetGeneClassifier(\n",
    "    input_dim=len(refgenes),\n",
    "    output_dim=19\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "088070bc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.1458, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "        [0.8902, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample = next(iter(train_loader[0]))[0]\n",
    "sample = clean_sample(sample, refgenes, train_loader[0].dataset.columns)\n",
    "sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d3775abc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-2.3695, -1.6859,  0.3932,  0.7834,  0.5724,  0.1285,  2.0458, -0.5353,\n",
       "          0.1072,  2.2878, -2.6266, -0.0060, -1.3519, -0.2654,  0.2857,  1.3722,\n",
       "         -2.1647, -2.0168,  0.2252],\n",
       "        [ 1.7041, -0.0514, -1.1603,  0.8445,  0.3909, -0.5014,  0.8765,  0.3303,\n",
       "          0.6350,  0.0367,  0.6937, -0.2794,  0.1761, -0.3965, -0.6405,  0.9846,\n",
       "         -1.6215,  0.5126,  1.2953],\n",
       "        [ 0.4198, -1.7357, -1.4540,  0.9354, -0.1871,  0.1724,  1.1644, -1.0471,\n",
       "          0.1651, -0.5473, -1.2712, -0.4917, -1.0942, -0.3880,  0.5926,  0.2460,\n",
       "         -1.1279,  0.5290, -0.1490],\n",
       "        [ 0.3247,  0.0626, -0.4729, -0.0760, -0.9306,  0.1010,  2.7008, -1.3465,\n",
       "          0.4771,  0.3002, -0.5355, -0.5808, -1.0911, -0.2043,  0.5783, -0.8588,\n",
       "         -0.1487, -0.3219,  0.8527]], grad_fn=<MmBackward0>)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c2124a9",
   "metadata": {},
   "source": [
    "Now, we'll subset and define our DataLoaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "31975f3d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mjlehrer1\u001b[0m (use `wandb login --relogin` to force relogin)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.12.11"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/Users/julian/Documents/Projects/organoid-classification/notebooks/wandb/run-20220404_111127-1eseo35v</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/jlehrer1/organoid-classification-notebooks/runs/1eseo35v\" target=\"_blank\">fanciful-wind-38</a></strong> to <a href=\"https://wandb.ai/jlehrer1/organoid-classification-notebooks\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "On loader idx = 0\n",
      "On minibatch i = 0/10\n",
      "On minibatch i = 1/10\n",
      "On minibatch i = 2/10\n",
      "On minibatch i = 3/10\n",
      "On minibatch i = 4/10\n",
      "On minibatch i = 5/10\n",
      "On minibatch i = 6/10\n",
      "On minibatch i = 7/10\n",
      "On minibatch i = 8/10\n",
      "On minibatch i = 9/10\n",
      "On minibatch i = 10/10\n",
      "On loader idx = 1\n",
      "On minibatch i = 0/10\n",
      "On minibatch i = 1/10\n",
      "On minibatch i = 2/10\n",
      "On minibatch i = 3/10\n",
      "On minibatch i = 4/10\n",
      "On minibatch i = 5/10\n",
      "On minibatch i = 6/10\n",
      "On minibatch i = 7/10\n",
      "On minibatch i = 8/10\n",
      "On minibatch i = 9/10\n",
      "On minibatch i = 10/10\n",
      "On loader idx = 2\n",
      "On minibatch i = 0/10\n",
      "On minibatch i = 1/10\n",
      "On minibatch i = 2/10\n",
      "On minibatch i = 3/10\n",
      "On minibatch i = 4/10\n",
      "On minibatch i = 5/10\n",
      "On minibatch i = 6/10\n",
      "On minibatch i = 7/10\n",
      "On minibatch i = 8/10\n",
      "On minibatch i = 9/10\n",
      "On minibatch i = 10/10\n",
      "On loader idx = 3\n",
      "On minibatch i = 0/10\n",
      "On minibatch i = 1/10\n",
      "On minibatch i = 2/10\n",
      "On minibatch i = 3/10\n",
      "On minibatch i = 4/10\n",
      "On minibatch i = 5/10\n",
      "On minibatch i = 6/10\n",
      "On minibatch i = 7/10\n",
      "On minibatch i = 8/10\n",
      "On minibatch i = 9/10\n",
      "On minibatch i = 10/10\n",
      "On loader i = 10\n",
      "On loader i = 10\n",
      "On loader i = 10\n",
      "On loader i = 10\n"
     ]
    }
   ],
   "source": [
    "import wandb\n",
    "from torchmetrics.functional import accuracy\n",
    "\n",
    "wandb.init()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\n",
    "\n",
    "train_loss = []\n",
    "val_loss = []\n",
    "test_loss = []\n",
    "\n",
    "mod = 10\n",
    "wandb.watch(model)\n",
    "for epoch in range(1):  # loop over the dataset multiple times\n",
    "    running_loss = 0.0\n",
    "    epoch_loss = 0.0\n",
    "    # Train loop\n",
    "    model.train()\n",
    "    for idx, train in enumerate(train_loader):\n",
    "        print(f'On loader {idx = }')\n",
    "        for i, data in enumerate(train):\n",
    "            print(f'On minibatch {i = }/10')\n",
    "            if i == 10:\n",
    "                break \n",
    "            inputs, labels = data\n",
    "            # CLEAN INPUTS\n",
    "            inputs = clean_sample(inputs, refgenes, train.dataset.columns)\n",
    "            # Forward pass ➡\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            # Backward pass ⬅\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "\n",
    "            # Step with optimizer\n",
    "            optimizer.step()\n",
    "\n",
    "            # print statistics\n",
    "            running_loss += loss.item()\n",
    "            epoch_loss += loss.item()\n",
    "\n",
    "\n",
    "            if i % mod == 0: # record every 2000 mini batches \n",
    "                metric_results = calculate_metrics(\n",
    "                    outputs=outputs,\n",
    "                    labels=labels,\n",
    "                    append_str='train',\n",
    "                    num_classes=model.output_dim,\n",
    "                    subset='weighted_accuracy',\n",
    "                )\n",
    "\n",
    "                wandb.log(metric_results)\n",
    "                running_loss = running_loss / mod\n",
    "                wandb.log({f\"batch_train_loss\": loss})\n",
    "\n",
    "                running_loss = 0.0\n",
    "            \n",
    "    wandb.log({f\"epoch_train_loss\": epoch_loss / len(train)})\n",
    "    \n",
    "    model.eval()\n",
    "    with torch.no_grad(): # save memory but not computing gradients \n",
    "        running_loss = 0.0\n",
    "        epoch_loss = 0.0\n",
    "        \n",
    "        for val in val_loader:\n",
    "            print(f'On loader {i = }')\n",
    "            for i, data in enumerate(val):\n",
    "                if i == 10:\n",
    "                    break \n",
    "                inputs, labels = data\n",
    "                # CLEAN INPUTS\n",
    "                inputs = clean_sample(inputs, refgenes, val.dataset.columns)\n",
    "                # Forward pass ➡\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "\n",
    "                # print statistics\n",
    "                running_loss += loss.item()\n",
    "                epoch_loss += loss.item()\n",
    "\n",
    "                if i % mod == 0: #every 2000 mini batches \n",
    "                    running_loss = running_loss / mod\n",
    "                    wandb.log({\"val_loss\": loss})\n",
    "                    running_loss = 0.0\n",
    "\n",
    "                    metric_results = calculate_metrics(\n",
    "                        outputs=outputs,\n",
    "                        labels=labels,\n",
    "                        num_classes=model.output_dim,\n",
    "                        subset='weighted_accuracy',\n",
    "                        append_str='val',\n",
    "                    )\n",
    "\n",
    "                wandb.log(metric_results)\n",
    "    \n",
    "        wandb.log({f\"epoch_val_loss\": epoch_loss / len(train)})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0e65554e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TabNetGeneClassifier(\n",
       "  (embedder): EmbeddingGenerator()\n",
       "  (tabnet): TabNetNoEmbeddings(\n",
       "    (initial_bn): BatchNorm1d(16604, eps=1e-05, momentum=0.01, affine=True, track_running_stats=True)\n",
       "    (encoder): TabNetEncoder(\n",
       "      (initial_bn): BatchNorm1d(16604, eps=1e-05, momentum=0.01, affine=True, track_running_stats=True)\n",
       "      (initial_splitter): FeatTransformer(\n",
       "        (shared): GLU_Block(\n",
       "          (shared_layers): ModuleList(\n",
       "            (0): Linear(in_features=16604, out_features=32, bias=False)\n",
       "            (1): Linear(in_features=16, out_features=32, bias=False)\n",
       "          )\n",
       "          (glu_layers): ModuleList(\n",
       "            (0): GLU_Layer(\n",
       "              (fc): Linear(in_features=16604, out_features=32, bias=False)\n",
       "              (bn): GBN(\n",
       "                (bn): BatchNorm1d(32, eps=1e-05, momentum=0.02, affine=True, track_running_stats=True)\n",
       "              )\n",
       "            )\n",
       "            (1): GLU_Layer(\n",
       "              (fc): Linear(in_features=16, out_features=32, bias=False)\n",
       "              (bn): GBN(\n",
       "                (bn): BatchNorm1d(32, eps=1e-05, momentum=0.02, affine=True, track_running_stats=True)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (specifics): GLU_Block(\n",
       "          (glu_layers): ModuleList(\n",
       "            (0): GLU_Layer(\n",
       "              (fc): Linear(in_features=16, out_features=32, bias=False)\n",
       "              (bn): GBN(\n",
       "                (bn): BatchNorm1d(32, eps=1e-05, momentum=0.02, affine=True, track_running_stats=True)\n",
       "              )\n",
       "            )\n",
       "            (1): GLU_Layer(\n",
       "              (fc): Linear(in_features=16, out_features=32, bias=False)\n",
       "              (bn): GBN(\n",
       "                (bn): BatchNorm1d(32, eps=1e-05, momentum=0.02, affine=True, track_running_stats=True)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (feat_transformers): ModuleList(\n",
       "        (0): FeatTransformer(\n",
       "          (shared): GLU_Block(\n",
       "            (shared_layers): ModuleList(\n",
       "              (0): Linear(in_features=16604, out_features=32, bias=False)\n",
       "              (1): Linear(in_features=16, out_features=32, bias=False)\n",
       "            )\n",
       "            (glu_layers): ModuleList(\n",
       "              (0): GLU_Layer(\n",
       "                (fc): Linear(in_features=16604, out_features=32, bias=False)\n",
       "                (bn): GBN(\n",
       "                  (bn): BatchNorm1d(32, eps=1e-05, momentum=0.02, affine=True, track_running_stats=True)\n",
       "                )\n",
       "              )\n",
       "              (1): GLU_Layer(\n",
       "                (fc): Linear(in_features=16, out_features=32, bias=False)\n",
       "                (bn): GBN(\n",
       "                  (bn): BatchNorm1d(32, eps=1e-05, momentum=0.02, affine=True, track_running_stats=True)\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (specifics): GLU_Block(\n",
       "            (glu_layers): ModuleList(\n",
       "              (0): GLU_Layer(\n",
       "                (fc): Linear(in_features=16, out_features=32, bias=False)\n",
       "                (bn): GBN(\n",
       "                  (bn): BatchNorm1d(32, eps=1e-05, momentum=0.02, affine=True, track_running_stats=True)\n",
       "                )\n",
       "              )\n",
       "              (1): GLU_Layer(\n",
       "                (fc): Linear(in_features=16, out_features=32, bias=False)\n",
       "                (bn): GBN(\n",
       "                  (bn): BatchNorm1d(32, eps=1e-05, momentum=0.02, affine=True, track_running_stats=True)\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (1): FeatTransformer(\n",
       "          (shared): GLU_Block(\n",
       "            (shared_layers): ModuleList(\n",
       "              (0): Linear(in_features=16604, out_features=32, bias=False)\n",
       "              (1): Linear(in_features=16, out_features=32, bias=False)\n",
       "            )\n",
       "            (glu_layers): ModuleList(\n",
       "              (0): GLU_Layer(\n",
       "                (fc): Linear(in_features=16604, out_features=32, bias=False)\n",
       "                (bn): GBN(\n",
       "                  (bn): BatchNorm1d(32, eps=1e-05, momentum=0.02, affine=True, track_running_stats=True)\n",
       "                )\n",
       "              )\n",
       "              (1): GLU_Layer(\n",
       "                (fc): Linear(in_features=16, out_features=32, bias=False)\n",
       "                (bn): GBN(\n",
       "                  (bn): BatchNorm1d(32, eps=1e-05, momentum=0.02, affine=True, track_running_stats=True)\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (specifics): GLU_Block(\n",
       "            (glu_layers): ModuleList(\n",
       "              (0): GLU_Layer(\n",
       "                (fc): Linear(in_features=16, out_features=32, bias=False)\n",
       "                (bn): GBN(\n",
       "                  (bn): BatchNorm1d(32, eps=1e-05, momentum=0.02, affine=True, track_running_stats=True)\n",
       "                )\n",
       "              )\n",
       "              (1): GLU_Layer(\n",
       "                (fc): Linear(in_features=16, out_features=32, bias=False)\n",
       "                (bn): GBN(\n",
       "                  (bn): BatchNorm1d(32, eps=1e-05, momentum=0.02, affine=True, track_running_stats=True)\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (2): FeatTransformer(\n",
       "          (shared): GLU_Block(\n",
       "            (shared_layers): ModuleList(\n",
       "              (0): Linear(in_features=16604, out_features=32, bias=False)\n",
       "              (1): Linear(in_features=16, out_features=32, bias=False)\n",
       "            )\n",
       "            (glu_layers): ModuleList(\n",
       "              (0): GLU_Layer(\n",
       "                (fc): Linear(in_features=16604, out_features=32, bias=False)\n",
       "                (bn): GBN(\n",
       "                  (bn): BatchNorm1d(32, eps=1e-05, momentum=0.02, affine=True, track_running_stats=True)\n",
       "                )\n",
       "              )\n",
       "              (1): GLU_Layer(\n",
       "                (fc): Linear(in_features=16, out_features=32, bias=False)\n",
       "                (bn): GBN(\n",
       "                  (bn): BatchNorm1d(32, eps=1e-05, momentum=0.02, affine=True, track_running_stats=True)\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (specifics): GLU_Block(\n",
       "            (glu_layers): ModuleList(\n",
       "              (0): GLU_Layer(\n",
       "                (fc): Linear(in_features=16, out_features=32, bias=False)\n",
       "                (bn): GBN(\n",
       "                  (bn): BatchNorm1d(32, eps=1e-05, momentum=0.02, affine=True, track_running_stats=True)\n",
       "                )\n",
       "              )\n",
       "              (1): GLU_Layer(\n",
       "                (fc): Linear(in_features=16, out_features=32, bias=False)\n",
       "                (bn): GBN(\n",
       "                  (bn): BatchNorm1d(32, eps=1e-05, momentum=0.02, affine=True, track_running_stats=True)\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (att_transformers): ModuleList(\n",
       "        (0): AttentiveTransformer(\n",
       "          (fc): Linear(in_features=8, out_features=16604, bias=False)\n",
       "          (bn): GBN(\n",
       "            (bn): BatchNorm1d(16604, eps=1e-05, momentum=0.02, affine=True, track_running_stats=True)\n",
       "          )\n",
       "          (selector): Sparsemax()\n",
       "        )\n",
       "        (1): AttentiveTransformer(\n",
       "          (fc): Linear(in_features=8, out_features=16604, bias=False)\n",
       "          (bn): GBN(\n",
       "            (bn): BatchNorm1d(16604, eps=1e-05, momentum=0.02, affine=True, track_running_stats=True)\n",
       "          )\n",
       "          (selector): Sparsemax()\n",
       "        )\n",
       "        (2): AttentiveTransformer(\n",
       "          (fc): Linear(in_features=8, out_features=16604, bias=False)\n",
       "          (bn): GBN(\n",
       "            (bn): BatchNorm1d(16604, eps=1e-05, momentum=0.02, affine=True, track_running_stats=True)\n",
       "          )\n",
       "          (selector): Sparsemax()\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (final_mapping): Linear(in_features=8, out_features=19, bias=False)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f1daf999",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def test_loop(\n",
    "    model,\n",
    "    testloaders,\n",
    "    refgenes,\n",
    "    criterion,\n",
    "    mod,\n",
    "):\n",
    "    model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for idx, test in enumerate(testloaders):\n",
    "            print(f'On {idx = }')\n",
    "            running_loss = 0.0\n",
    "            for i, data in enumerate(test):\n",
    "                print(f'minibatch {i = }')\n",
    "                if i == 10:\n",
    "                    break\n",
    "                inputs, labels = data\n",
    "                # CLEAN INPUTS\n",
    "                inputs = clean_sample(inputs, refgenes, test.dataset.columns)\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "\n",
    "                # print statistics\n",
    "                running_loss += loss.item()\n",
    "                if i % mod == 0: #every 2000 mini batches \n",
    "                    running_loss = running_loss / mod\n",
    "                    wandb.log({\"test_loss\": loss})\n",
    "                    running_loss = 0.0\n",
    "\n",
    "                    metric_results = calculate_metrics(\n",
    "                        outputs=outputs,\n",
    "                        labels=labels,\n",
    "                        num_classes=model.output_dim,\n",
    "                        subset='weighted_accuracy',\n",
    "                        append_str='test',\n",
    "                    )\n",
    "\n",
    "                    wandb.log(metric_results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e407a6c1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "On idx = 0\n",
      "minibatch i = 0\n",
      "minibatch i = 1\n",
      "minibatch i = 2\n",
      "minibatch i = 3\n",
      "minibatch i = 4\n",
      "minibatch i = 5\n",
      "minibatch i = 6\n",
      "minibatch i = 7\n",
      "minibatch i = 8\n",
      "minibatch i = 9\n",
      "minibatch i = 10\n",
      "On idx = 1\n",
      "minibatch i = 0\n",
      "minibatch i = 1\n",
      "minibatch i = 2\n",
      "minibatch i = 3\n",
      "minibatch i = 4\n",
      "minibatch i = 5\n",
      "minibatch i = 6\n",
      "minibatch i = 7\n",
      "minibatch i = 8\n",
      "minibatch i = 9\n",
      "minibatch i = 10\n",
      "On idx = 2\n",
      "minibatch i = 0\n",
      "minibatch i = 1\n",
      "minibatch i = 2\n",
      "minibatch i = 3\n",
      "minibatch i = 4\n",
      "minibatch i = 5\n",
      "minibatch i = 6\n",
      "minibatch i = 7\n",
      "minibatch i = 8\n",
      "minibatch i = 9\n",
      "minibatch i = 10\n",
      "On idx = 3\n",
      "minibatch i = 0\n",
      "minibatch i = 1\n",
      "minibatch i = 2\n",
      "minibatch i = 3\n",
      "minibatch i = 4\n",
      "minibatch i = 5\n",
      "minibatch i = 6\n",
      "minibatch i = 7\n",
      "minibatch i = 8\n",
      "minibatch i = 9\n",
      "minibatch i = 10\n"
     ]
    }
   ],
   "source": [
    "test_loop(model, test_loader, refgenes, criterion, mod)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e7666179",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt \n",
    "\n",
    "plt.plot(train_loss, label='Train')\n",
    "plt.plot(val_loss, label='Val')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dab9b869",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "aabc85c6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['../data/processed/labels/primary_bhaduri_labels.csv',\n",
       " '../data/processed/labels/allen_cortex_labels.csv',\n",
       " '../data/processed/labels/allen_m1_region_labels.csv',\n",
       " '../data/processed/labels/whole_brain_bhaduri_labels.csv']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labelfiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6aa90a42",
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import memmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b9c67a6a",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Size of available data is not a multiple of the data-type size.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/pd/jsjcl0fn7w57s5mfr34b20pm0000gn/T/ipykernel_11070/2415507392.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmemmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'../data/interim/allen_cortex_T.csv'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat64\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'r'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/miniconda3/envs/base-data-science/lib/python3.9/site-packages/numpy/core/memmap.py\u001b[0m in \u001b[0;36m__new__\u001b[0;34m(subtype, filename, dtype, mode, offset, shape, order)\u001b[0m\n\u001b[1;32m    237\u001b[0m                 \u001b[0mbytes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mflen\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0moffset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    238\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mbytes\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0m_dbytes\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 239\u001b[0;31m                     raise ValueError(\"Size of available data is not a \"\n\u001b[0m\u001b[1;32m    240\u001b[0m                             \"multiple of the data-type size.\")\n\u001b[1;32m    241\u001b[0m                 \u001b[0msize\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbytes\u001b[0m \u001b[0;34m//\u001b[0m \u001b[0m_dbytes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Size of available data is not a multiple of the data-type size."
     ]
    }
   ],
   "source": [
    "f = memmap('../data/interim/allen_cortex_T.csv', dtype=np.float64, mode='r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6505026a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base-data-science]",
   "language": "python",
   "name": "conda-env-base-data-science-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
