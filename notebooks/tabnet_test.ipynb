{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "58f6ea9a",
   "metadata": {},
   "source": [
    "# TabNet Model Test\n",
    "\n",
    "In this notebook, we'll test a training loop for the TabNet model \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "eb9edaf4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['../data/interim/primary_bhaduri_T.csv',\n",
       "  '../data/interim/allen_cortex_T.csv',\n",
       "  '../data/interim/allen_m1_region_T.csv',\n",
       "  '../data/interim/whole_brain_bhaduri_T.csv'],\n",
       " ['../data/processed/labels/primary_bhaduri_labels.csv',\n",
       "  '../data/processed/labels/allen_cortex_labels.csv',\n",
       "  '../data/processed/labels/allen_m1_region_labels.csv',\n",
       "  '../data/processed/labels/whole_brain_bhaduri_labels.csv'])"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('../src')\n",
    "\n",
    "from models.lib.neural import *\n",
    "from models.lib.data import *\n",
    "from models.lib.train import *\n",
    "\n",
    "import helper \n",
    "from helper import gene_intersection\n",
    "from pytorch_tabnet.tab_network import TabNet\n",
    "\n",
    "import torch.nn as nn \n",
    "import torch.optim as optim\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "from torch.utils.data import Subset\n",
    "from helper import seed_everything\n",
    "\n",
    "seed_everything(42)\n",
    "t = helper.INTERIM_DATA_AND_LABEL_FILES_LIST\n",
    "datafiles, labelfiles = zip(*t.items())\n",
    "datafiles = [f'../data/interim/{f}' for f in datafiles]\n",
    "labelfiles = [f'../data/processed/labels/{f}' for f in labelfiles]\n",
    "refgenes = gene_intersection()\n",
    "\n",
    "datafiles, labelfiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bb4a8cde",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytorch_lightning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7e117d9",
   "metadata": {},
   "source": [
    "First, we'll define our train, val and test sets, then generate the associated DataLoaders and try training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f15f491c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.lib.neural import TabNetGeneClassifier\n",
    "\n",
    "model = TabNetGeneClassifier(\n",
    "    input_dim=len(refgenes),\n",
    "    output_dim=19\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d3775abc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SampleLoader(torch.utils.data.DataLoader):\n",
    "    def __init__(self, refgenes, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.refgenes = refgenes\n",
    "        self.currgenes = self.dataset.columns \n",
    "            \n",
    "    def __iter__(self):\n",
    "        for batch in super().__iter__():\n",
    "            yield clean_sample(batch[0], self.refgenes, self.currgenes), batch[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b1f6d065",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# test = SampleLoader(refgenes=refgenes, dataset=train, batch_size=11, num_workers=0)\n",
    "# next(iter(test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fbc26eda",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# next(iter(test))[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "76ef9ad2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# from functools import partial \n",
    "\n",
    "# def custom_collate(sample):\n",
    "#     data = torch.stack([x[0] for x in sample])\n",
    "#     labels = torch.tensor([x[1] for x in sample])\n",
    "#     return data, labels\n",
    "\n",
    "# test = DataLoader(dataset=train, collate_fn=custom_collate, batch_size=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ac2fbb6",
   "metadata": {},
   "source": [
    "## PyTorch-Lightning compatible TabNet architecture "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "6dd564eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.lib.neural import TabNetGeneClassifier\n",
    "import torch.functional as F\n",
    "from torchmetrics.functional import accuracy, precision, recall \n",
    "\n",
    "base_model = TabNetGeneClassifier(\n",
    "    input_dim=len(refgenes),\n",
    "    output_dim=19,\n",
    ")\n",
    "\n",
    "class GeneClassifier(pl.LightningModule):\n",
    "    \"\"\"\n",
    "        Initialize the gene classifier neural network\n",
    "\n",
    "        Parameters:\n",
    "        input_dim: Number of features in the inpute matrix \n",
    "        output_dim: Number of classes\n",
    "        weights: Weights to use in loss calculation to account for imbalance in class size \n",
    "        params: Dictionary of hyperparameters to use. Must include width, layers, lr, momentum, weight_decay\n",
    "        metrics: Dictionary of metrics to log, where keys are metric names and values are torchmetrics.functional methods\n",
    "        weighted_metrics: If True, use class-weighted calculation in metrics. Otherwise, use default 'micro' calculation.\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self, \n",
    "        input_dim, \n",
    "        output_dim,\n",
    "        base_model=None,\n",
    "        optimizer=torch.optim.Adam,\n",
    "        optim_params: Dict[str, float]={\n",
    "            'lr': 0.001,\n",
    "            'weight_decay': 0.01,\n",
    "        },\n",
    "        metrics: Dict[str, Callable]={\n",
    "            'accuracy': accuracy,\n",
    "            'precision': precision,\n",
    "            'recall': recall,\n",
    "        },\n",
    "        weighted_metrics=False,\n",
    "        *args,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        print(f'Model initialized. {input_dim = }, {output_dim = }. Metrics are {metrics.keys()} and {weighted_metrics = }')\n",
    "\n",
    "        if base_model is None:\n",
    "            self.base_model = TabNetGeneClassifier(\n",
    "                input_dim=input_dim,\n",
    "                output_dim=output_dim,\n",
    "                *args,\n",
    "                **kwargs,\n",
    "            )\n",
    "        else:\n",
    "            self.base_model = base_model\n",
    "        \n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = output_dim\n",
    "        self.optimizer = optimizer\n",
    "        self.optim_params = optim_params\n",
    "        self.metrics = metrics\n",
    "        self.weighted_metrics = weighted_metrics\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = self.base_model(x)\n",
    "        return out\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        y_hat = self(x)\n",
    "        \n",
    "        loss = F.cross_entropy(y_hat, y)\n",
    "        \n",
    "        self.log(\"train_loss\", loss, logger=True, on_epoch=True, on_step=True)\n",
    "        self._compute_metrics(y_hat, y, 'train')\n",
    "        return loss\n",
    "    \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        y_hat = self(x)\n",
    "        val_loss = F.cross_entropy(y_hat, y)\n",
    "\n",
    "        self.log(\"val_loss\", val_loss, logger=True, on_epoch=True, on_step=True)\n",
    "        self._compute_metrics(y_hat, y, 'val')\n",
    "        \n",
    "        return val_loss\n",
    "    \n",
    "    def _compute_metrics(self, y_hat, y, tag, on_epoch=True, on_step=True):\n",
    "        for name, metric in self.metrics.items():\n",
    "            if not self.weighted_metrics: # We dont consider class support in calculation\n",
    "                val = metric(y_hat, y, average='weighted', num_classes=self.output_dim)\n",
    "                self.log(\n",
    "                    f\"weighted_{tag}_{name}\", \n",
    "                    val, \n",
    "                    on_epoch=on_epoch, \n",
    "                    on_step=on_step,\n",
    "                    logger=True,\n",
    "                )\n",
    "            else:\n",
    "                val = metric(y_hat, y)\n",
    "                self.log(\n",
    "                    f\"{tag}_{name}\", \n",
    "                    val, \n",
    "                    on_epoch=on_epoch, \n",
    "                    on_step=on_step,\n",
    "                    logger=True,\n",
    "                )\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = self.optimizer(self.parameters(), **self.optim_params)\n",
    "        return optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "e34934de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16604"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "base_model.input_dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "6a510881",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model initialized. input_dim = 16604, output_dim = 19. Metrics are dict_keys(['accuracy', 'precision', 'recall']) and weighted_metrics = False\n"
     ]
    }
   ],
   "source": [
    "classifier = GeneClassifier(\n",
    "    input_dim=len(refgenes),\n",
    "    output_dim=19,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "2dfac175",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytorch_lightning as pl \n",
    "from typing import *\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from itertools import chain\n",
    "\n",
    "def custom_collate(sample, refgenes, currgenes):\n",
    "    data = clean_sample(torch.stack([x[0] for x in sample]), refgenes, currgenes)\n",
    "    labels = torch.tensor([x[1] for x in sample])\n",
    "    return data, labels\n",
    "\n",
    "class CollateLoader(torch.utils.data.DataLoader):\n",
    "    def __init__(self, refgenes, currgenes, *args, **kwargs):\n",
    "        collate_fn = functools.partial(custom_collate, refgenes=refgenes, currgenes=currgenes)\n",
    "        super().__init__(collate_fn = collate_fn, *args, **kwargs)\n",
    "\n",
    "class SequentialLoader:\n",
    "    def __init__(self, dataloaders):\n",
    "        self.dataloaders = dataloaders\n",
    "\n",
    "    def __len__(self):\n",
    "        return sum([len(dl) for dl in self.dataloaders])\n",
    "\n",
    "    def __iter__(self):\n",
    "        yield from chain(*self.dataloaders)\n",
    "                \n",
    "class GeneDataModule(pl.LightningDataModule):\n",
    "    def __init__(\n",
    "        self, \n",
    "        datafiles: List[str],\n",
    "        labelfiles: List[str],\n",
    "        class_label: str,\n",
    "        refgenes: List[str],\n",
    "        batch_size: int=16,\n",
    "        num_workers=32,\n",
    "        *args,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.__dict__.update(**kwargs)\n",
    "\n",
    "        self.datafiles = datafiles\n",
    "        self.labelfiles = labelfiles\n",
    "        self.class_label = class_label\n",
    "        self.refgenes = refgenes\n",
    "        \n",
    "        self.num_workers = num_workers\n",
    "        self.batch_size = batch_size\n",
    "        \n",
    "        self.trainloaders = []\n",
    "        self.valloaders = []\n",
    "        self.testloaders = []\n",
    "        \n",
    "        self.args = args\n",
    "        self.kwargs = kwargs\n",
    "        \n",
    "    def prepare_data(self):\n",
    "        # Download data from S3 here \n",
    "        pass \n",
    "    \n",
    "    def setup(self, stage: Optional[str] = None):\n",
    "        trainloaders, valloaders, testloaders = generate_loaders(\n",
    "            datafiles=self.datafiles,\n",
    "            labelfiles=self.labelfiles,\n",
    "            class_label=self.class_label,\n",
    "            refgenes=self.refgenes,\n",
    "            batch_size=self.batch_size,\n",
    "            num_workers=self.num_workers,\n",
    "            *self.args,\n",
    "            **self.kwargs\n",
    "        )\n",
    "        \n",
    "        self.trainloaders = SequentialLoader(trainloaders)\n",
    "        self.valloaders = SequentialLoader(valloaders)\n",
    "        self.testloaders = SequentialLoader(testloaders)\n",
    "        \n",
    "    def train_dataloader(self):\n",
    "        return self.trainloaders\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return self.valloaders\n",
    "\n",
    "    def test_dataloader(self):\n",
    "        return self.testloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "e824c545",
   "metadata": {},
   "outputs": [],
   "source": [
    "module = GeneDataModule(\n",
    "    datafiles=datafiles, \n",
    "    labelfiles=labelfiles, \n",
    "    class_label='Type', \n",
    "    refgenes=refgenes,\n",
    "    skip=3, \n",
    "    normalize=True,\n",
    "    batch_size=8,\n",
    "    num_workers=0,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "d50005bd",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "/Users/julian/miniconda3/envs/base-data-science/lib/python3.9/site-packages/pytorch_lightning/loggers/wandb.py:341: UserWarning: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.\n",
      "  rank_zero_warn(\n",
      "\n",
      "  | Name       | Type                 | Params\n",
      "----------------------------------------------------\n",
      "0 | base_model | TabNetGeneClassifier | 1.1 M \n",
      "----------------------------------------------------\n",
      "1.1 M     Trainable params\n",
      "0         Non-trainable params\n",
      "1.1 M     Total params\n",
      "4.407     Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation sanity check: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "047ee4995d8340cfb78c109194bfbe8c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/julian/miniconda3/envs/base-data-science/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py:685: UserWarning: Detected KeyboardInterrupt, attempting graceful shutdown...\n",
      "  rank_zero_warn(\"Detected KeyboardInterrupt, attempting graceful shutdown...\")\n"
     ]
    }
   ],
   "source": [
    "from pytorch_lightning import Trainer \n",
    "from pytorch_lightning.loggers import WandbLogger\n",
    "\n",
    "wandb_logger = WandbLogger(name='Tabnet with Metrics')\n",
    "trainer = Trainer(logger=wandb_logger)\n",
    "\n",
    "trainer.fit(classifier, datamodule=module)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "ceb9becd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error in callback <function _WandbInit._resume_backend at 0x7fcdbdc09b80> (for pre_run_cell):\n"
     ]
    },
    {
     "ename": "Exception",
     "evalue": "The wandb backend process has shutdown",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mException\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m~/miniconda3/envs/base-data-science/lib/python3.9/site-packages/backcall/backcall.py\u001b[0m in \u001b[0;36madapted\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    102\u001b[0m                 \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m \u001b[0;31m#            print(args, kwargs, unmatched_pos, cut_positional, unmatched_kw)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 104\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    105\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0madapted\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/base-data-science/lib/python3.9/site-packages/wandb/sdk/wandb_init.py\u001b[0m in \u001b[0;36m_resume_backend\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    326\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackend\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    327\u001b[0m             \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"resuming backend\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minterface\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpublish_resume\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    329\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_jupyter_teardown\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/base-data-science/lib/python3.9/site-packages/wandb/sdk/interface/interface.py\u001b[0m in \u001b[0;36mpublish_resume\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    579\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mpublish_resume\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    580\u001b[0m         \u001b[0mresume\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mResumeRequest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 581\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_publish_resume\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresume\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    582\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    583\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mabstractmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/base-data-science/lib/python3.9/site-packages/wandb/sdk/interface/interface_shared.py\u001b[0m in \u001b[0;36m_publish_resume\u001b[0;34m(self, resume)\u001b[0m\n\u001b[1;32m    277\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_publish_resume\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresume\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mpb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mResumeRequest\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    278\u001b[0m         \u001b[0mrec\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_request\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresume\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mresume\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 279\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_publish\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    280\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    281\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_publish_run\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mpb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRunRecord\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/base-data-science/lib/python3.9/site-packages/wandb/sdk/interface/interface_queue.py\u001b[0m in \u001b[0;36m_publish\u001b[0;34m(self, record, local)\u001b[0m\n\u001b[1;32m     47\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_publish\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecord\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m\"pb.Record\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlocal\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_process_check\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_process\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_process\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_alive\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"The wandb backend process has shutdown\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlocal\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m             \u001b[0mrecord\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlocal\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlocal\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mException\u001b[0m: The wandb backend process has shutdown"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'primary_bhaduri_T.csv': 'primary_bhaduri_labels.csv',\n",
       " 'allen_cortex_T.csv': 'allen_cortex_labels.csv',\n",
       " 'allen_m1_region_T.csv': 'allen_m1_region_labels.csv',\n",
       " 'whole_brain_bhaduri_T.csv': 'whole_brain_bhaduri_labels.csv'}"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error in callback <function _WandbInit._pause_backend at 0x7fcdbdc09d30> (for post_run_cell):\n"
     ]
    },
    {
     "ename": "Exception",
     "evalue": "The wandb backend process has shutdown",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mException\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m~/miniconda3/envs/base-data-science/lib/python3.9/site-packages/backcall/backcall.py\u001b[0m in \u001b[0;36madapted\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    102\u001b[0m                 \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m \u001b[0;31m#            print(args, kwargs, unmatched_pos, cut_positional, unmatched_kw)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 104\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    105\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0madapted\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/base-data-science/lib/python3.9/site-packages/wandb/sdk/wandb_init.py\u001b[0m in \u001b[0;36m_pause_backend\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    321\u001b[0m                 \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog_code\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mroot\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    322\u001b[0m                 \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"saved code: %s\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mres\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 323\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minterface\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpublish_pause\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    324\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    325\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_resume_backend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/base-data-science/lib/python3.9/site-packages/wandb/sdk/interface/interface.py\u001b[0m in \u001b[0;36mpublish_pause\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    571\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mpublish_pause\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    572\u001b[0m         \u001b[0mpause\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPauseRequest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 573\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_publish_pause\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpause\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    574\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    575\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mabstractmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/base-data-science/lib/python3.9/site-packages/wandb/sdk/interface/interface_shared.py\u001b[0m in \u001b[0;36m_publish_pause\u001b[0;34m(self, pause)\u001b[0m\n\u001b[1;32m    273\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_publish_pause\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpause\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mpb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPauseRequest\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    274\u001b[0m         \u001b[0mrec\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_request\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpause\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpause\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 275\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_publish\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    276\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    277\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_publish_resume\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresume\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mpb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mResumeRequest\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/base-data-science/lib/python3.9/site-packages/wandb/sdk/interface/interface_queue.py\u001b[0m in \u001b[0;36m_publish\u001b[0;34m(self, record, local)\u001b[0m\n\u001b[1;32m     47\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_publish\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecord\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m\"pb.Record\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlocal\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_process_check\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_process\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_process\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_alive\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"The wandb backend process has shutdown\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlocal\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m             \u001b[0mrecord\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlocal\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlocal\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mException\u001b[0m: The wandb backend process has shutdown"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('../src')\n",
    "import helper\n",
    "helper.INTERIM_DATA_AND_LABEL_FILES_LIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1ec01a0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base-data-science]",
   "language": "python",
   "name": "conda-env-base-data-science-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
