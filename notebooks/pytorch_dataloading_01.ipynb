{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a44fa48e",
   "metadata": {},
   "source": [
    "# Dataloading 01\n",
    "\n",
    "In this notebook, we'll figure out how to use PyTorch's DataLoader class to load our massive files without reading the entirety of them into memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4594d799",
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask.dataframe as dd\n",
    "import pandas as pd \n",
    "import torch\n",
    "import linecache \n",
    "import csv\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d13416c2",
   "metadata": {},
   "source": [
    "We'll first design a custom dataset to use with PyTorch's `DataLoader` class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9b719349",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GeneExpressionData(Dataset):\n",
    "    def __init__(self, filename, labelname):\n",
    "        self._filename = filename\n",
    "        self._labelname = labelname\n",
    "        self._total_data = 0\n",
    "        \n",
    "        with open(filename, \"r\") as f:\n",
    "            self._total_data = len(f.readlines()) - 1\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        if idx == 0:\n",
    "            return self.__getitem__(1)\n",
    "        \n",
    "        line = linecache.getline(self._filename, idx + 1)\n",
    "        label = linecache.getline(self._labelname, idx + 1)\n",
    "        \n",
    "        csv_data = csv.reader([line])\n",
    "        csv_label = csv.reader([label])\n",
    "        \n",
    "        data = [x for x in csv_data][0]\n",
    "        label = [x for x in csv_label][0]\n",
    "        \n",
    "        return (\n",
    "            torch.from_numpy(np.array([float(x) for x in data])),\n",
    "            torch.from_numpy(np.array([int(float(x)) for x in label])),\n",
    "        )\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self._total_data\n",
    "    \n",
    "    def num_labels(self):\n",
    "        return pd.read_csv(self._labelname)['# label'].nunique()\n",
    "    \n",
    "    def num_features(self):\n",
    "        return len(self.__getitem__(0)[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba34c06c",
   "metadata": {},
   "source": [
    "Since PyTorch loss functions require classes in $[0, C]$, we'll first add $1$ to the labels and re-write it out so we can use it for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "adfa8e5b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def fix_labels(file):\n",
    "    labels = pd.read_csv(file)\n",
    "    labels['# label'] = labels['# label'].astype(int) + 1\n",
    "    labels.to_csv('fixed_' + file.split('/')[-1], index=False)\n",
    "\n",
    "# fix_labels('../data/processed/primary_reduction_neighbors_10_components_3.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4b36ee2",
   "metadata": {},
   "source": [
    "Let's test this quickly and then continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "75d8ae26",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0,  6,  5,  9,  3,  1,  2, 14, 15, 18, 16, 10, 13, 12,  8,  4,  7,\n",
       "       17, 11, 19])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fixed_labels = pd.read_csv('fixed_primary_labels_neighbors_15_components_100_clust_size_100.csv')\n",
    "fixed_labels['# label'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4c4601c",
   "metadata": {},
   "source": [
    "Great, we now continue as normal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "64a7b2bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "t = GeneExpressionData(\n",
    "    filename='../data/processed/primary_reduction_neighbors_10_components_3.csv',\n",
    "    labelname='fixed_primary_labels_neighbors_15_components_100_clust_size_100.csv'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b6d2387a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.num_features()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98dcf8c2",
   "metadata": {},
   "source": [
    "Before we train our model, we need to split our data into training and testing sets, in order to get an unbiased evaluation of our model's performance. Likely, we will initially overfit the training set since we provide no regularization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f1beffe0",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size = int(0.8 * len(t))\n",
    "test_size = len(t) - train_size\n",
    "\n",
    "train, test = torch.utils.data.random_split(t, [train_size, test_size])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "72553951",
   "metadata": {},
   "outputs": [],
   "source": [
    "traindata = DataLoader(train, batch_size = 64, num_workers = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d2efd2a",
   "metadata": {},
   "source": [
    "Now that we've defined our `DataLoader`, let's test it when training a simple Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "16f9cf50",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NN(nn.Module):\n",
    "    def __init__(self, N_features, N_labels):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.network = nn.Sequential(\n",
    "            nn.Linear(in_features=N_features, out_features=16),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(in_features=16, out_features=32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(in_features=32, out_features=16),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(in_features=16, out_features=N_labels),\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.network(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "63c3d04c",
   "metadata": {},
   "outputs": [],
   "source": [
    "network = NN(\n",
    "    N_features=t.num_features(),\n",
    "    N_labels=t.num_labels()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0eae48a",
   "metadata": {},
   "source": [
    "Now we can define our criterion, optimization method and train our model on our dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ff488c04",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(network.parameters(), lr = 0.01)\n",
    "loss_arr = []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ebeea40",
   "metadata": {},
   "source": [
    "And finally train our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7f977c39",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 has loss 2.9951248168945312\n",
      "Epoch 3 has loss 2.928852081298828\n",
      "Epoch 4 has loss 2.815742015838623\n",
      "Epoch 5 has loss 2.760014772415161\n",
      "Epoch 6 has loss 2.55064058303833\n",
      "Epoch 7 has loss 2.5633928775787354\n",
      "Epoch 8 has loss 2.4664502143859863\n",
      "Epoch 9 has loss 2.2520246505737305\n",
      "Epoch 10 has loss 2.168370246887207\n",
      "Epoch 11 has loss 2.09975266456604\n",
      "Epoch 12 has loss 2.21954607963562\n",
      "Epoch 13 has loss 2.2362663745880127\n",
      "Epoch 14 has loss 2.0091419219970703\n",
      "Epoch 15 has loss 1.973549723625183\n",
      "Epoch 16 has loss 1.9382860660552979\n",
      "Epoch 17 has loss 2.110527515411377\n",
      "Epoch 18 has loss 1.9396253824234009\n",
      "Epoch 19 has loss 2.0526015758514404\n",
      "Epoch 20 has loss 1.9217842817306519\n",
      "Epoch 21 has loss 1.8541680574417114\n",
      "Epoch 22 has loss 1.7285218238830566\n",
      "Epoch 23 has loss 1.870560646057129\n",
      "Epoch 24 has loss 1.8828296661376953\n",
      "Epoch 25 has loss 1.7172486782073975\n",
      "Epoch 26 has loss 1.8873571157455444\n",
      "Epoch 27 has loss 1.723275899887085\n",
      "Epoch 28 has loss 1.7409210205078125\n",
      "Epoch 29 has loss 1.8110849857330322\n",
      "Epoch 30 has loss 1.631894826889038\n",
      "Epoch 31 has loss 1.897812843322754\n",
      "Epoch 32 has loss 1.4159897565841675\n",
      "Epoch 33 has loss 1.677287220954895\n",
      "Epoch 34 has loss 1.6560016870498657\n",
      "Epoch 35 has loss 1.4557448625564575\n",
      "Epoch 36 has loss 1.339807391166687\n",
      "Epoch 37 has loss 1.257352590560913\n",
      "Epoch 38 has loss 1.3658628463745117\n",
      "Epoch 39 has loss 1.1996469497680664\n",
      "Epoch 40 has loss 1.5685343742370605\n",
      "Epoch 41 has loss 1.3711017370224\n",
      "Epoch 42 has loss 1.4384524822235107\n",
      "Epoch 43 has loss 1.2937579154968262\n",
      "Epoch 44 has loss 1.283159852027893\n",
      "Epoch 45 has loss 1.3472578525543213\n",
      "Epoch 46 has loss 1.1741751432418823\n",
      "Epoch 47 has loss 1.3273320198059082\n",
      "Epoch 48 has loss 1.0640616416931152\n",
      "Epoch 49 has loss 1.4076329469680786\n"
     ]
    }
   ],
   "source": [
    "epochs = 50\n",
    "\n",
    "for i in range(1, epochs):\n",
    "    if i > 1 and i %% 10 == 0:\n",
    "        print(f'Epoch {i} has loss {loss_arr[i]}')\n",
    "        \n",
    "    for X, y in traindata:\n",
    "        yhat = network(X.float())\n",
    "        loss = criterion(yhat, y.flatten())\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        loss_arr.append(loss.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c572ce0",
   "metadata": {},
   "source": [
    "Now, let's test our model on the test set and evaluate our results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "67ef6c92",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "Dimension out of range (expected to be in range of [-1, 0], but got 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/pd/jsjcl0fn7w57s5mfr34b20pm0000gn/T/ipykernel_11711/2672011152.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m#     output = output.data.numpy()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0mprediction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprediction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: Dimension out of range (expected to be in range of [-1, 0], but got 1)"
     ]
    }
   ],
   "source": [
    "test_loss = []\n",
    "\n",
    "for X, y in test:\n",
    "    output = network(X.float())\n",
    "#     output = output.data.numpy()\n",
    "    \n",
    "    prediction = int(torch.max(output.data, 1)[1].numpy())\n",
    "    print(prediction)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c8f92b7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base-data-science] *",
   "language": "python",
   "name": "conda-env-base-data-science-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
