{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7267866e",
   "metadata": {},
   "source": [
    "# Dataloading 01\n",
    "\n",
    "In this notebook, we'll figure out how to use PyTorch's DataLoader class to load our massive files without reading the entirety of them into memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c111cd61",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7fbbb093be30>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import dask.dataframe as dd\n",
    "import pandas as pd \n",
    "import torch\n",
    "import linecache \n",
    "import csv\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "torch.manual_seed(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a75396dd",
   "metadata": {},
   "source": [
    "We'll first design a custom dataset to use with PyTorch's `DataLoader` class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9854304f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GeneExpressionData(Dataset):\n",
    "    def __init__(self, filename, labelname):\n",
    "        self._filename = filename\n",
    "        self._labelname = labelname\n",
    "        self._total_data = 0\n",
    "        \n",
    "        with open(filename, \"r\") as f:\n",
    "            self._total_data = len(f.readlines()) - 1\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        if idx == 0:\n",
    "            return self.__getitem__(1)\n",
    "        \n",
    "        line = linecache.getline(self._filename, idx + 1)\n",
    "        label = linecache.getline(self._labelname, idx + 1)\n",
    "        \n",
    "        csv_data = csv.reader([line])\n",
    "        csv_label = csv.reader([label])\n",
    "        \n",
    "        data = [x for x in csv_data][0]\n",
    "        label = [x for x in csv_label][0]\n",
    "        \n",
    "        return torch.from_numpy(np.array([float(x) for x in data])).float(), [int(float(x)) for x in label][0]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self._total_data\n",
    "    \n",
    "    def num_labels(self):\n",
    "        return pd.read_csv(self._labelname)['# label'].nunique()\n",
    "    \n",
    "    def num_features(self):\n",
    "        return len(self.__getitem__(0)[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d8b3611",
   "metadata": {},
   "source": [
    "Since PyTorch loss functions require classes in $[0, C]$, we'll first add $1$ to the labels and re-write it out so we can use it for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "21adce01",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def fix_labels(file):\n",
    "    labels = pd.read_csv(file)\n",
    "    labels['# label'] = labels['# label'].astype(int) + 1\n",
    "    labels.to_csv('fixed_' + file.split('/')[-1], index=False)\n",
    "\n",
    "fix_labels('../data/processed/primary_labels_neighbors_500_components_100_clust_size_100.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40a66373",
   "metadata": {},
   "source": [
    "Let's test this quickly and then continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c1d977fa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    123188\n",
       "0     64253\n",
       "2      1968\n",
       "Name: # label, dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fixed_labels = pd.read_csv('fixed_primary_labels_neighbors_500_components_100_clust_size_100.csv')\n",
    "fixed_labels['# label'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a8eecf6",
   "metadata": {},
   "source": [
    "Great, we now continue as normal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "de1c2186",
   "metadata": {},
   "outputs": [],
   "source": [
    "t = GeneExpressionData(\n",
    "    filename='../data/processed/primary_reduction_neighbors_500_components_100.csv',\n",
    "    labelname='fixed_primary_labels_neighbors_500_components_100_clust_size_250.csv'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1d7cddba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>...</th>\n",
       "      <th>90</th>\n",
       "      <th>91</th>\n",
       "      <th>92</th>\n",
       "      <th>93</th>\n",
       "      <th>94</th>\n",
       "      <th>95</th>\n",
       "      <th>96</th>\n",
       "      <th>97</th>\n",
       "      <th>98</th>\n",
       "      <th>99</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1.174407</td>\n",
       "      <td>4.605017</td>\n",
       "      <td>5.700520</td>\n",
       "      <td>4.349964</td>\n",
       "      <td>0.009240</td>\n",
       "      <td>4.443735</td>\n",
       "      <td>4.977513</td>\n",
       "      <td>1.097707</td>\n",
       "      <td>4.923816</td>\n",
       "      <td>...</td>\n",
       "      <td>1.977319</td>\n",
       "      <td>6.758616</td>\n",
       "      <td>3.999133</td>\n",
       "      <td>4.044898</td>\n",
       "      <td>4.987543</td>\n",
       "      <td>1.790490</td>\n",
       "      <td>4.197807</td>\n",
       "      <td>6.984423</td>\n",
       "      <td>9.260715</td>\n",
       "      <td>7.174916</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1.187639</td>\n",
       "      <td>4.608121</td>\n",
       "      <td>5.704485</td>\n",
       "      <td>4.363834</td>\n",
       "      <td>0.024695</td>\n",
       "      <td>4.443859</td>\n",
       "      <td>4.973516</td>\n",
       "      <td>1.105194</td>\n",
       "      <td>4.919911</td>\n",
       "      <td>...</td>\n",
       "      <td>1.981181</td>\n",
       "      <td>6.762886</td>\n",
       "      <td>3.999778</td>\n",
       "      <td>4.040472</td>\n",
       "      <td>4.999935</td>\n",
       "      <td>1.792208</td>\n",
       "      <td>4.192843</td>\n",
       "      <td>6.986716</td>\n",
       "      <td>9.258625</td>\n",
       "      <td>7.172100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>1.179722</td>\n",
       "      <td>4.631567</td>\n",
       "      <td>5.731539</td>\n",
       "      <td>4.270429</td>\n",
       "      <td>0.007747</td>\n",
       "      <td>4.434293</td>\n",
       "      <td>4.971396</td>\n",
       "      <td>1.129670</td>\n",
       "      <td>4.909443</td>\n",
       "      <td>...</td>\n",
       "      <td>2.006164</td>\n",
       "      <td>6.765180</td>\n",
       "      <td>4.005325</td>\n",
       "      <td>4.039588</td>\n",
       "      <td>5.014343</td>\n",
       "      <td>1.786922</td>\n",
       "      <td>4.206350</td>\n",
       "      <td>6.986751</td>\n",
       "      <td>9.256733</td>\n",
       "      <td>7.179493</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>1.233760</td>\n",
       "      <td>4.637250</td>\n",
       "      <td>5.735640</td>\n",
       "      <td>4.311743</td>\n",
       "      <td>0.078151</td>\n",
       "      <td>4.433172</td>\n",
       "      <td>4.963012</td>\n",
       "      <td>1.207390</td>\n",
       "      <td>4.900370</td>\n",
       "      <td>...</td>\n",
       "      <td>2.000760</td>\n",
       "      <td>6.769579</td>\n",
       "      <td>3.998335</td>\n",
       "      <td>4.026979</td>\n",
       "      <td>4.999375</td>\n",
       "      <td>1.817481</td>\n",
       "      <td>4.194461</td>\n",
       "      <td>6.987063</td>\n",
       "      <td>9.243600</td>\n",
       "      <td>7.164564</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>1.188722</td>\n",
       "      <td>4.624152</td>\n",
       "      <td>5.721036</td>\n",
       "      <td>4.324148</td>\n",
       "      <td>0.035837</td>\n",
       "      <td>4.437555</td>\n",
       "      <td>4.966181</td>\n",
       "      <td>1.132194</td>\n",
       "      <td>4.908939</td>\n",
       "      <td>...</td>\n",
       "      <td>1.998834</td>\n",
       "      <td>6.764676</td>\n",
       "      <td>4.001324</td>\n",
       "      <td>4.034964</td>\n",
       "      <td>5.004535</td>\n",
       "      <td>1.799192</td>\n",
       "      <td>4.201400</td>\n",
       "      <td>6.986776</td>\n",
       "      <td>9.252455</td>\n",
       "      <td>7.172283</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>189404</th>\n",
       "      <td>189404</td>\n",
       "      <td>1.181309</td>\n",
       "      <td>4.649603</td>\n",
       "      <td>5.754858</td>\n",
       "      <td>4.248226</td>\n",
       "      <td>-0.014871</td>\n",
       "      <td>4.430346</td>\n",
       "      <td>4.971585</td>\n",
       "      <td>1.140923</td>\n",
       "      <td>4.899366</td>\n",
       "      <td>...</td>\n",
       "      <td>2.013938</td>\n",
       "      <td>6.766751</td>\n",
       "      <td>4.011445</td>\n",
       "      <td>4.043420</td>\n",
       "      <td>5.018497</td>\n",
       "      <td>1.773362</td>\n",
       "      <td>4.211319</td>\n",
       "      <td>6.986798</td>\n",
       "      <td>9.257227</td>\n",
       "      <td>7.180955</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>189405</th>\n",
       "      <td>189405</td>\n",
       "      <td>1.181091</td>\n",
       "      <td>4.658228</td>\n",
       "      <td>5.758984</td>\n",
       "      <td>4.250365</td>\n",
       "      <td>-0.020029</td>\n",
       "      <td>4.426039</td>\n",
       "      <td>4.967492</td>\n",
       "      <td>1.133520</td>\n",
       "      <td>4.895007</td>\n",
       "      <td>...</td>\n",
       "      <td>2.023807</td>\n",
       "      <td>6.771064</td>\n",
       "      <td>4.014005</td>\n",
       "      <td>4.036175</td>\n",
       "      <td>5.034298</td>\n",
       "      <td>1.776111</td>\n",
       "      <td>4.210604</td>\n",
       "      <td>6.986619</td>\n",
       "      <td>9.255919</td>\n",
       "      <td>7.182779</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>189406</th>\n",
       "      <td>189406</td>\n",
       "      <td>1.176980</td>\n",
       "      <td>4.635231</td>\n",
       "      <td>5.733450</td>\n",
       "      <td>4.225388</td>\n",
       "      <td>-0.008851</td>\n",
       "      <td>4.433665</td>\n",
       "      <td>4.974230</td>\n",
       "      <td>1.149803</td>\n",
       "      <td>4.905263</td>\n",
       "      <td>...</td>\n",
       "      <td>2.008648</td>\n",
       "      <td>6.768187</td>\n",
       "      <td>4.008596</td>\n",
       "      <td>4.042135</td>\n",
       "      <td>5.014659</td>\n",
       "      <td>1.782834</td>\n",
       "      <td>4.212437</td>\n",
       "      <td>6.986119</td>\n",
       "      <td>9.254880</td>\n",
       "      <td>7.180373</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>189407</th>\n",
       "      <td>189407</td>\n",
       "      <td>1.175157</td>\n",
       "      <td>4.637198</td>\n",
       "      <td>5.734905</td>\n",
       "      <td>4.307137</td>\n",
       "      <td>-0.013657</td>\n",
       "      <td>4.434302</td>\n",
       "      <td>4.975091</td>\n",
       "      <td>1.114408</td>\n",
       "      <td>4.909545</td>\n",
       "      <td>...</td>\n",
       "      <td>1.996098</td>\n",
       "      <td>6.758251</td>\n",
       "      <td>4.008240</td>\n",
       "      <td>4.046164</td>\n",
       "      <td>4.986329</td>\n",
       "      <td>1.776909</td>\n",
       "      <td>4.210703</td>\n",
       "      <td>6.980882</td>\n",
       "      <td>9.256554</td>\n",
       "      <td>7.178543</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>189408</th>\n",
       "      <td>189408</td>\n",
       "      <td>1.144102</td>\n",
       "      <td>4.623600</td>\n",
       "      <td>5.719321</td>\n",
       "      <td>4.208792</td>\n",
       "      <td>-0.025483</td>\n",
       "      <td>4.433794</td>\n",
       "      <td>4.969801</td>\n",
       "      <td>1.120868</td>\n",
       "      <td>4.909011</td>\n",
       "      <td>...</td>\n",
       "      <td>2.011456</td>\n",
       "      <td>6.767829</td>\n",
       "      <td>4.011205</td>\n",
       "      <td>4.039267</td>\n",
       "      <td>5.016262</td>\n",
       "      <td>1.781148</td>\n",
       "      <td>4.214013</td>\n",
       "      <td>6.985889</td>\n",
       "      <td>9.252168</td>\n",
       "      <td>7.180094</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>189409 rows × 101 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        Unnamed: 0         0         1         2         3         4  \\\n",
       "0                0  1.174407  4.605017  5.700520  4.349964  0.009240   \n",
       "1                1  1.187639  4.608121  5.704485  4.363834  0.024695   \n",
       "2                2  1.179722  4.631567  5.731539  4.270429  0.007747   \n",
       "3                3  1.233760  4.637250  5.735640  4.311743  0.078151   \n",
       "4                4  1.188722  4.624152  5.721036  4.324148  0.035837   \n",
       "...            ...       ...       ...       ...       ...       ...   \n",
       "189404      189404  1.181309  4.649603  5.754858  4.248226 -0.014871   \n",
       "189405      189405  1.181091  4.658228  5.758984  4.250365 -0.020029   \n",
       "189406      189406  1.176980  4.635231  5.733450  4.225388 -0.008851   \n",
       "189407      189407  1.175157  4.637198  5.734905  4.307137 -0.013657   \n",
       "189408      189408  1.144102  4.623600  5.719321  4.208792 -0.025483   \n",
       "\n",
       "               5         6         7         8  ...        90        91  \\\n",
       "0       4.443735  4.977513  1.097707  4.923816  ...  1.977319  6.758616   \n",
       "1       4.443859  4.973516  1.105194  4.919911  ...  1.981181  6.762886   \n",
       "2       4.434293  4.971396  1.129670  4.909443  ...  2.006164  6.765180   \n",
       "3       4.433172  4.963012  1.207390  4.900370  ...  2.000760  6.769579   \n",
       "4       4.437555  4.966181  1.132194  4.908939  ...  1.998834  6.764676   \n",
       "...          ...       ...       ...       ...  ...       ...       ...   \n",
       "189404  4.430346  4.971585  1.140923  4.899366  ...  2.013938  6.766751   \n",
       "189405  4.426039  4.967492  1.133520  4.895007  ...  2.023807  6.771064   \n",
       "189406  4.433665  4.974230  1.149803  4.905263  ...  2.008648  6.768187   \n",
       "189407  4.434302  4.975091  1.114408  4.909545  ...  1.996098  6.758251   \n",
       "189408  4.433794  4.969801  1.120868  4.909011  ...  2.011456  6.767829   \n",
       "\n",
       "              92        93        94        95        96        97        98  \\\n",
       "0       3.999133  4.044898  4.987543  1.790490  4.197807  6.984423  9.260715   \n",
       "1       3.999778  4.040472  4.999935  1.792208  4.192843  6.986716  9.258625   \n",
       "2       4.005325  4.039588  5.014343  1.786922  4.206350  6.986751  9.256733   \n",
       "3       3.998335  4.026979  4.999375  1.817481  4.194461  6.987063  9.243600   \n",
       "4       4.001324  4.034964  5.004535  1.799192  4.201400  6.986776  9.252455   \n",
       "...          ...       ...       ...       ...       ...       ...       ...   \n",
       "189404  4.011445  4.043420  5.018497  1.773362  4.211319  6.986798  9.257227   \n",
       "189405  4.014005  4.036175  5.034298  1.776111  4.210604  6.986619  9.255919   \n",
       "189406  4.008596  4.042135  5.014659  1.782834  4.212437  6.986119  9.254880   \n",
       "189407  4.008240  4.046164  4.986329  1.776909  4.210703  6.980882  9.256554   \n",
       "189408  4.011205  4.039267  5.016262  1.781148  4.214013  6.985889  9.252168   \n",
       "\n",
       "              99  \n",
       "0       7.174916  \n",
       "1       7.172100  \n",
       "2       7.179493  \n",
       "3       7.164564  \n",
       "4       7.172283  \n",
       "...          ...  \n",
       "189404  7.180955  \n",
       "189405  7.182779  \n",
       "189406  7.180373  \n",
       "189407  7.178543  \n",
       "189408  7.180094  \n",
       "\n",
       "[189409 rows x 101 columns]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('../data/processed/primary_reduction_neighbors_500_components_100.csv')\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e449b7b",
   "metadata": {},
   "source": [
    "Let's see how fast it takes to load a minibatch of data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "372f593c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 110 ms, sys: 49.7 ms, total: 159 ms\n",
      "Wall time: 158 ms\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "\n",
    "for i in range(64):\n",
    "    t.__getitem__(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0d406e85",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.num_labels()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8092856",
   "metadata": {},
   "source": [
    "Before we train our model, we need to split our data into training and testing sets, in order to get an unbiased evaluation of our model's performance. Likely, we will initially overfit the training set since we provide no regularization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "78575cc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size = int(0.8 * len(t))\n",
    "test_size = len(t) - train_size\n",
    "\n",
    "train, test = torch.utils.data.random_split(t, [train_size, test_size])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "d216d1b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "traindata = DataLoader(train, batch_size = 8, num_workers = 0)\n",
    "valdata = DataLoader(test, batch_size = 8, num_workers = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6695113b",
   "metadata": {},
   "source": [
    "Now that we've defined our `DataLoader`, let's test it when training a simple Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "021a73e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NN(nn.Module):\n",
    "    def __init__(self, N_features, N_labels):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.network = nn.Sequential(\n",
    "            nn.BatchNorm1d(num_features=N_features),\n",
    "            nn.Linear(in_features=N_features, out_features=16),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(in_features=16, out_features=32),\n",
    "            nn.ReLU(),\n",
    "#             nn.Conv1d(in_channels=32, out_channels=8, kernel_size=1),\n",
    "            nn.Linear(in_features=32, out_features=64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(in_features=64, out_features=N_labels),\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.network(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "88f01656",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 101, 1, 1])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = NN(\n",
    "    N_features=t.num_features(),\n",
    "    N_labels=t.num_labels()\n",
    ")\n",
    "\n",
    "m = torch.randn(10, t.num_features(), 1)\n",
    "\n",
    "m.unsqueeze(dim=3).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e49a68a",
   "metadata": {},
   "source": [
    "Now we can define our criterion, optimization method and train our model on our dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "4588c153",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr = 0.01)\n",
    "loss_arr = []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4159fe20",
   "metadata": {},
   "source": [
    "And finally train our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "0cc52753",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from ignite.engine import Events, create_supervised_trainer, create_supervised_evaluator\n",
    "from ignite.metrics import Accuracy, Loss, ClassificationReport\n",
    "\n",
    "trainer = create_supervised_trainer(model, optimizer, criterion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "7e3994b0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'traindata' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/pd/jsjcl0fn7w57s5mfr34b20pm0000gn/T/ipykernel_15201/4266666406.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtraindata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m         \u001b[0;31m# zero the parameter gradients\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'traindata' is not defined"
     ]
    }
   ],
   "source": [
    "epochs = 100000\n",
    "\n",
    "for i in range(epochs):\n",
    "    model.train()\n",
    "\n",
    "    for X, y in traindata:\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        outputs = model(X)\n",
    "        loss = criterion(outputs, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        loss_arr.append(loss.item())\n",
    "        \n",
    "    print(f'Epoch {i} is {loss_arr[i]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78718088",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1.0587053998627307\n",
    "# 1.058727260653217\n",
    "# 1.0583432531826011"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39e498da",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5adfef9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base-data-science] *",
   "language": "python",
   "name": "conda-env-base-data-science-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
