{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7267866e",
   "metadata": {},
   "source": [
    "# Dataloading 01\n",
    "\n",
    "In this notebook, we'll figure out how to use PyTorch's DataLoader class to load our massive files without reading the entirety of them into memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c111cd61",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7fe1304dde70>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import dask.dataframe as dd\n",
    "import pandas as pd \n",
    "import torch\n",
    "import linecache \n",
    "import csv\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "torch.manual_seed(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a75396dd",
   "metadata": {},
   "source": [
    "We'll first design a custom dataset to use with PyTorch's `DataLoader` class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9854304f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GeneExpressionData(Dataset):\n",
    "    def __init__(self, filename, labelname):\n",
    "        self._filename = filename\n",
    "        self._labelname = labelname\n",
    "        self._total_data = 0\n",
    "        \n",
    "        with open(filename, \"r\") as f:\n",
    "            self._total_data = len(f.readlines()) - 1\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        line = linecache.getline(self._filename, idx + 2)\n",
    "        label = linecache.getline(self._labelname, idx + 2)\n",
    "        \n",
    "        csv_data = csv.reader([line])\n",
    "        csv_label = csv.reader([label])\n",
    "        \n",
    "        data = [x for x in csv_data][0]\n",
    "        label = [x for x in csv_label][0]\n",
    "        \n",
    "        return torch.from_numpy(np.array([float(x) for x in data[1:]])).float(), [int(float(x)) for x in label][0]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self._total_data\n",
    "    \n",
    "    def num_labels(self):\n",
    "        return pd.read_csv(self._labelname)['# label'].nunique()\n",
    "    \n",
    "    def num_features(self):\n",
    "        return len(self.__getitem__(0)[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d8b3611",
   "metadata": {},
   "source": [
    "Since PyTorch loss functions require classes in $[0, C]$, we'll first add $1$ to the labels and re-write it out so we can use it for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "21adce01",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def fix_labels(file):\n",
    "    labels = pd.read_csv(file)\n",
    "    labels['# label'] = labels['# label'].astype(int) + 1\n",
    "    labels.to_csv('fixed_' + file.split('/')[-1], index=False)\n",
    "\n",
    "fix_labels('../data/processed/labels/primary_labels_neighbors_50_components_50_clust_size_100.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a8eecf6",
   "metadata": {},
   "source": [
    "Great, we now continue as normal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "de1c2186",
   "metadata": {},
   "outputs": [],
   "source": [
    "t = GeneExpressionData(\n",
    "    filename='../data/processed/pca/pca_components_50_primary.csv',\n",
    "    labelname='fixed_primary_labels_neighbors_50_components_50_clust_size_100.csv'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d0e2c395",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([-0.7190,  3.1106, -2.7917, -0.5684,  0.8418, -3.6876,  0.8462,  0.8769,\n",
       "          0.8501, -1.2519,  0.2484, -0.1826, -1.0845,  1.9183, -0.0116,  0.9930,\n",
       "         -0.2720, -0.1639, -0.1635,  0.4015, -0.3665,  0.4014, -0.6133, -0.0702,\n",
       "          0.3934,  1.2825, -0.5827,  0.1928, -0.3193, -0.6941,  0.0094,  0.6268,\n",
       "         -0.4087,  0.7147,  1.3372,  0.7491,  0.0941,  0.9887, -0.8340, -1.0065,\n",
       "         -0.1222, -1.0664, -0.1622, -0.3615,  0.7603, -0.9312, -0.1044, -0.4603,\n",
       "          0.1306]),\n",
       " 3)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.__getitem__(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e449b7b",
   "metadata": {},
   "source": [
    "Let's see how fast it takes to load a minibatch of data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "372f593c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3.35 ms, sys: 1.18 ms, total: 4.53 ms\n",
      "Wall time: 3.52 ms\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "\n",
    "for i in range(64):\n",
    "    t.__getitem__(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8092856",
   "metadata": {},
   "source": [
    "Before we train our model, we need to split our data into training and testing sets, in order to get an unbiased evaluation of our model's performance. Likely, we will initially overfit the training set since we provide no regularization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "78575cc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size = int(0.8 * len(t))\n",
    "test_size = len(t) - train_size\n",
    "\n",
    "train, test = torch.utils.data.random_split(t, [train_size, test_size])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d216d1b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "traindata = DataLoader(train, batch_size = 64, num_workers = 0)\n",
    "valdata = DataLoader(test, batch_size = 64, num_workers = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6695113b",
   "metadata": {},
   "source": [
    "Now that we've defined our `DataLoader`, let's test it when training a simple Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "021a73e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NN(nn.Module):\n",
    "    def __init__(self, N_features, N_labels):\n",
    "        super(NN, self).__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(N_features, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, N_labels),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        logits = self.linear_relu_stack(x)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "88f01656",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = NN(\n",
    "    N_features=t.num_features(),\n",
    "    N_labels=t.num_labels()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e49a68a",
   "metadata": {},
   "source": [
    "Now we can define our criterion, optimization method and train our model on our dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4588c153",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr = 0.01)\n",
    "loss_arr = []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4159fe20",
   "metadata": {},
   "source": [
    "And finally train our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7e3994b0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from ignite.engine import Events, create_supervised_trainer, create_supervised_evaluator\n",
    "from ignite.metrics import Precision, Recall, Loss\n",
    "\n",
    "# model = NN()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "trainer = create_supervised_trainer(model, optimizer, criterion)\n",
    "\n",
    "val_metrics = {\n",
    "    \"precision\": Precision(),\n",
    "    \"recall\": Recall(),\n",
    "    \"crossentropy\": Loss(criterion)\n",
    "}\n",
    "\n",
    "evaluator = create_supervised_evaluator(model, metrics=val_metrics)\n",
    "\n",
    "log_interval = 1\n",
    "\n",
    "@trainer.on(Events.EPOCH_COMPLETED)\n",
    "def log_training_results(trainer):\n",
    "    evaluator.run(traindata)\n",
    "    metrics = evaluator.state.metrics\n",
    "    \n",
    "    print(\n",
    "        f\"Training Results - Epoch: {trainer.state.epoch}\\\n",
    "        Avg loss: {metrics['crossentropy']}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78718088",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Results - Epoch: 1        Avg loss: 2.2940958954509756\n",
      "Training Results - Epoch: 2        Avg loss: 2.293602996495674\n",
      "Training Results - Epoch: 3        Avg loss: 2.2933361298646444\n"
     ]
    }
   ],
   "source": [
    "trainer.run(traindata, max_epochs=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39e498da",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5adfef9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base-data-science]",
   "language": "python",
   "name": "conda-env-base-data-science-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
