{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7267866e",
   "metadata": {},
   "source": [
    "# Dataloading 01\n",
    "\n",
    "In this notebook, we'll figure out how to use PyTorch's DataLoader class to load our massive files without reading the entirety of them into memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "c111cd61",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7fe810ac7cd0>"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import comet_ml\n",
    "import dask.dataframe as dd\n",
    "import pandas as pd \n",
    "import torch\n",
    "import linecache \n",
    "import csv\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import pytorch_lightning as pl\n",
    "import torch.nn.functional as F\n",
    "\n",
    "torch.manual_seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "a42583b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LightningNN(pl.LightningModule):\n",
    "    def __init__(self, N_features, N_labels):\n",
    "        super(LightningNN, self).__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(N_features, 512),\n",
    "            nn.ReLU(),\n",
    "            *layers,\n",
    "            nn.Linear(64, N_labels),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        logits = self.linear_relu_stack(x)\n",
    "        return logits\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=1e-3)\n",
    "        return optimizer\n",
    "    \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        y_hat = self(x)\n",
    "        loss = F.cross_entropy(y_hat, y)\n",
    "        self.log(\"train_loss\", loss, on_step=True, on_epoch=True, logger=True)\n",
    "        return loss\n",
    "    \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        y_hat = self(x)\n",
    "        val_loss = F.cross_entropy(y_hat, y)\n",
    "        self.log(\"val_loss\", val_loss, on_step=True, on_epoch=True, logger=True)\n",
    "        return val_loss\n",
    "    \n",
    "model = LightningNN(3,3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a75396dd",
   "metadata": {},
   "source": [
    "We'll first design a custom dataset to use with PyTorch's `DataLoader` class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "9854304f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GeneExpressionData(Dataset):\n",
    "    def __init__(self, filename, labelname):\n",
    "        self._filename = filename\n",
    "        self._labelname = labelname\n",
    "        self._total_data = 0\n",
    "        \n",
    "        with open(filename, \"r\") as f:\n",
    "            self._total_data = len(f.readlines()) - 1\n",
    "    \n",
    "    def __getitem__(self, idx):        \n",
    "        line = linecache.getline(self._filename, idx + 2)\n",
    "        label = linecache.getline(self._labelname, idx + 2)\n",
    "        \n",
    "        csv_data = csv.reader([line])\n",
    "        csv_label = csv.reader([label])\n",
    "        \n",
    "        data = [x for x in csv_data][0]\n",
    "        label = [x for x in csv_label][0]\n",
    "        return torch.from_numpy(np.array([float(x) for x in data])).float(), [int(float(x)) for x in label][0]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self._total_data\n",
    "    \n",
    "    def num_labels(self):\n",
    "        return pd.read_csv(self._labelname)['# label'].nunique()\n",
    "    \n",
    "    def num_features(self):\n",
    "        return len(self.__getitem__(0)[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d8b3611",
   "metadata": {},
   "source": [
    "Since PyTorch loss functions require classes in $[0, C]$, we'll first add $1$ to the labels and re-write it out so we can use it for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "21adce01",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def fix_labels(file):\n",
    "    labels = pd.read_csv(file)\n",
    "    labels['# label'] = labels['# label'].astype(int) + 1\n",
    "    labels.to_csv('fixed_' + file.split('/')[-1], index=False)\n",
    "\n",
    "fix_labels('../data/processed/labels/primary_labels_neighbors_50_components_50_clust_size_100.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a8eecf6",
   "metadata": {},
   "source": [
    "Great, we now continue as normal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "de1c2186",
   "metadata": {},
   "outputs": [],
   "source": [
    "t = GeneExpressionData(\n",
    "    filename='../data/processed/umap/primary_reduction_neighbors_100_components_3.csv',\n",
    "    labelname='fixed_primary_labels_neighbors_50_components_50_clust_size_100.csv'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e449b7b",
   "metadata": {},
   "source": [
    "Let's see how fast it takes to load a minibatch of data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "372f593c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3.88 ms, sys: 2.64 ms, total: 6.52 ms\n",
      "Wall time: 5.17 ms\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "\n",
    "for i in range(64):\n",
    "    t.__getitem__(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8092856",
   "metadata": {},
   "source": [
    "Before we train our model, we need to split our data into training and testing sets, in order to get an unbiased evaluation of our model's performance. Likely, we will initially overfit the training set since we provide no regularization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "78575cc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size = int(0.8 * len(t))\n",
    "test_size = len(t) - train_size\n",
    "\n",
    "train, test = torch.utils.data.random_split(t, [train_size, test_size])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "d216d1b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "traindata = DataLoader(train, batch_size = 8, num_workers = 0)\n",
    "valdata = DataLoader(test, batch_size = 8, num_workers = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6695113b",
   "metadata": {},
   "source": [
    "Now that we've defined our `DataLoader`, let's test it when training a simple Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a88c448e",
   "metadata": {},
   "source": [
    "## Using PyTorch Lightning\n",
    "\n",
    "PyTorch lightning seems nicer than Ignite, especially for GPU training. Let's test it out "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "b5adfef9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NN(pl.LightningModule):\n",
    "    def __init__(self, N_features, N_labels, weights):\n",
    "        super(NN, self).__init__()\n",
    "        self.weights = weights\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(N_features, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 1024),\n",
    "            nn.ReLU(),\n",
    "            *layers,\n",
    "            nn.Linear(1024, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, N_labels),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        logits = self.linear_relu_stack(x)\n",
    "        return logits\n",
    "    \n",
    "    def accuracy(self, y_hat, y):\n",
    "        y_hat = torch.argmax(y_hat, dim=1)\n",
    "        accuracy = torch.sum(y == y_hat).item() / (len(y) * 1.0)\n",
    "        return torch.tensor(accuracy)\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=1e-3)\n",
    "        return optimizer\n",
    "    \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        y_hat = self(x)\n",
    "        loss = F.cross_entropy(y_hat, y, weight=self.weights)\n",
    "        acc = self.accuracy(y_hat, y)\n",
    "\n",
    "        self.log(\"train_loss\", loss, on_step=True, on_epoch=True, logger=True)\n",
    "        self.log(\"train_accuracy\", acc, on_step=True, on_epoch=True, logger=True)\n",
    "        return loss\n",
    "    \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        y_hat = self(x)\n",
    "        val_loss = F.cross_entropy(y_hat, y, weight=self.weights)\n",
    "        acc = self.accuracy(y_hat, y)\n",
    "\n",
    "        self.log(\"val_loss\", val_loss, on_step=True, on_epoch=True, logger=True)\n",
    "        self.log(\"train_accuracy\", acc, on_step=True, on_epoch=True, logger=True)\n",
    "        return val_loss\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "db924463",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "def class_weights(label_df):\n",
    "    label_df = pd.read_csv(label_df)\n",
    "    \n",
    "    weights = compute_class_weight(\n",
    "        class_weight='balanced', \n",
    "        classes=np.unique(label_df), \n",
    "        y=label_df.values.reshape(-1)\n",
    "    ) \n",
    "\n",
    "    weights = torch.from_numpy(weights)\n",
    "\n",
    "    return weights.float()\n",
    "\n",
    "weights = class_weights('fixed_primary_labels_neighbors_50_components_50_clust_size_100.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "1b2a8982",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([  0.6428,   2.0281,  43.2046,   1.8374,   0.8580,  33.3467, 102.0523,\n",
       "        100.3226,   0.5029,   0.6207,   2.0534,   0.4522,   0.3983,  13.3462,\n",
       "          1.3319,   0.3946])"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "99926809",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = NN(t.num_features(), t.num_labels(), weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "536d2548",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "CometLogger will be initialized in online mode\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "COMET INFO: ---------------------------\n",
      "COMET INFO: Comet.ml Experiment Summary\n",
      "COMET INFO: ---------------------------\n",
      "COMET INFO:   Data:\n",
      "COMET INFO:     display_summary_level : 1\n",
      "COMET INFO:     url                   : https://www.comet.ml/jlehrer1/gene-expression-classification/ac4395be63614a59a6841c183dd3364c\n",
      "COMET INFO:   Others:\n",
      "COMET INFO:     Name : test2\n",
      "COMET INFO:   Uploads:\n",
      "COMET INFO:     environment details      : 1\n",
      "COMET INFO:     filename                 : 1\n",
      "COMET INFO:     git metadata             : 1\n",
      "COMET INFO:     git-patch (uncompressed) : 1 (8.18 KB)\n",
      "COMET INFO:     installed packages       : 1\n",
      "COMET INFO:     model graph              : 1\n",
      "COMET INFO:     notebook                 : 1\n",
      "COMET INFO:     source_code              : 1\n",
      "COMET INFO: ---------------------------\n",
      "COMET WARNING: As you are running in a Jupyter environment, you will need to call `experiment.end()` when finished to ensure all metrics and code are logged before exiting.\n",
      "COMET INFO: Experiment is live on comet.ml https://www.comet.ml/jlehrer1/gene-expression-classification/4ec9a4ea985b4eb6b85853258cc8affd\n",
      "\n",
      "\n",
      "  | Name              | Type       | Params\n",
      "-------------------------------------------------\n",
      "0 | flatten           | Flatten    | 0     \n",
      "1 | linear_relu_stack | Sequential | 2.1 M \n",
      "-------------------------------------------------\n",
      "2.1 M     Trainable params\n",
      "0         Non-trainable params\n",
      "2.1 M     Total params\n",
      "8.543     Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation sanity check: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d46cc5b8085241a3b81d551438116612",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "comet_logger = pl.loggers.CometLogger(\n",
    "        api_key=\"neMNyjJuhw25ao48JEWlJpKRR\",\n",
    "        project_name=\"gene-expression-classification\",  # Optional\n",
    "        experiment_name=\"test2\"\n",
    "    )\n",
    "\n",
    "\n",
    "trainer = pl.Trainer(auto_lr_find=True, max_epochs=10, logger=comet_logger)\n",
    "trainer.fit(model, traindata, valdata)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "a2a6705f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'train_loss': tensor(2.5939),\n",
       " 'train_loss_step': tensor(2.5939),\n",
       " 'train_accuracy': tensor(0.),\n",
       " 'train_accuracy_step': tensor(0.)}"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.callback_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d66e947",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base-data-science] *",
   "language": "python",
   "name": "conda-env-base-data-science-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
