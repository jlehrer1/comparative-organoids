{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Modeling 01\n",
    "\n",
    "In this notebook, we'll begin building the classifier to show that Layer 4 neurons do not exist in the organoid data. We will do this in the following manner.\n",
    "\n",
    "1. Identify cells in the primary data by which layer of the cortex they are in.\n",
    "2. Train a classifier on the primary data.\n",
    "3. Under the assumption that the space of gene expression is the same in organoids, classify the organoid cells to their respective cortex layer and show that none get classified as layer 4.\n",
    "4. Conclude that layer 4 cells do not exist in the organoid data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import matplotlib.pyplot as plt \n",
    "import numpy as np\n",
    "import umap\n",
    "import hdbscan\n",
    "from collections import Counter\n",
    "import seaborn as sns\n",
    "import plotly.express as px \n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "primary = pd.read_csv('primary_labels_test.csv')\n",
    "df = pd.read_csv('../data/processed/primary_reduction_neighbors_500_components_100.csv', index_col='Unnamed: 0')\n",
    "df['label'] = primary['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>91</th>\n",
       "      <th>92</th>\n",
       "      <th>93</th>\n",
       "      <th>94</th>\n",
       "      <th>95</th>\n",
       "      <th>96</th>\n",
       "      <th>97</th>\n",
       "      <th>98</th>\n",
       "      <th>99</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.174407</td>\n",
       "      <td>4.605017</td>\n",
       "      <td>5.700520</td>\n",
       "      <td>4.349964</td>\n",
       "      <td>0.009240</td>\n",
       "      <td>4.443735</td>\n",
       "      <td>4.977513</td>\n",
       "      <td>1.097707</td>\n",
       "      <td>4.923816</td>\n",
       "      <td>5.728352</td>\n",
       "      <td>...</td>\n",
       "      <td>6.758616</td>\n",
       "      <td>3.999133</td>\n",
       "      <td>4.044898</td>\n",
       "      <td>4.987543</td>\n",
       "      <td>1.790490</td>\n",
       "      <td>4.197807</td>\n",
       "      <td>6.984423</td>\n",
       "      <td>9.260715</td>\n",
       "      <td>7.174916</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.187639</td>\n",
       "      <td>4.608121</td>\n",
       "      <td>5.704485</td>\n",
       "      <td>4.363834</td>\n",
       "      <td>0.024695</td>\n",
       "      <td>4.443859</td>\n",
       "      <td>4.973516</td>\n",
       "      <td>1.105194</td>\n",
       "      <td>4.919911</td>\n",
       "      <td>5.728892</td>\n",
       "      <td>...</td>\n",
       "      <td>6.762886</td>\n",
       "      <td>3.999778</td>\n",
       "      <td>4.040472</td>\n",
       "      <td>4.999935</td>\n",
       "      <td>1.792208</td>\n",
       "      <td>4.192843</td>\n",
       "      <td>6.986716</td>\n",
       "      <td>9.258625</td>\n",
       "      <td>7.172100</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.179722</td>\n",
       "      <td>4.631567</td>\n",
       "      <td>5.731539</td>\n",
       "      <td>4.270429</td>\n",
       "      <td>0.007747</td>\n",
       "      <td>4.434293</td>\n",
       "      <td>4.971396</td>\n",
       "      <td>1.129670</td>\n",
       "      <td>4.909443</td>\n",
       "      <td>5.748357</td>\n",
       "      <td>...</td>\n",
       "      <td>6.765180</td>\n",
       "      <td>4.005325</td>\n",
       "      <td>4.039588</td>\n",
       "      <td>5.014343</td>\n",
       "      <td>1.786922</td>\n",
       "      <td>4.206350</td>\n",
       "      <td>6.986751</td>\n",
       "      <td>9.256733</td>\n",
       "      <td>7.179493</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.233760</td>\n",
       "      <td>4.637250</td>\n",
       "      <td>5.735640</td>\n",
       "      <td>4.311743</td>\n",
       "      <td>0.078151</td>\n",
       "      <td>4.433172</td>\n",
       "      <td>4.963012</td>\n",
       "      <td>1.207390</td>\n",
       "      <td>4.900370</td>\n",
       "      <td>5.740662</td>\n",
       "      <td>...</td>\n",
       "      <td>6.769579</td>\n",
       "      <td>3.998335</td>\n",
       "      <td>4.026979</td>\n",
       "      <td>4.999375</td>\n",
       "      <td>1.817481</td>\n",
       "      <td>4.194461</td>\n",
       "      <td>6.987063</td>\n",
       "      <td>9.243600</td>\n",
       "      <td>7.164564</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.188722</td>\n",
       "      <td>4.624152</td>\n",
       "      <td>5.721036</td>\n",
       "      <td>4.324148</td>\n",
       "      <td>0.035837</td>\n",
       "      <td>4.437555</td>\n",
       "      <td>4.966181</td>\n",
       "      <td>1.132194</td>\n",
       "      <td>4.908939</td>\n",
       "      <td>5.740864</td>\n",
       "      <td>...</td>\n",
       "      <td>6.764676</td>\n",
       "      <td>4.001324</td>\n",
       "      <td>4.034964</td>\n",
       "      <td>5.004535</td>\n",
       "      <td>1.799192</td>\n",
       "      <td>4.201400</td>\n",
       "      <td>6.986776</td>\n",
       "      <td>9.252455</td>\n",
       "      <td>7.172283</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 101 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          0         1         2         3         4         5         6  \\\n",
       "0  1.174407  4.605017  5.700520  4.349964  0.009240  4.443735  4.977513   \n",
       "1  1.187639  4.608121  5.704485  4.363834  0.024695  4.443859  4.973516   \n",
       "2  1.179722  4.631567  5.731539  4.270429  0.007747  4.434293  4.971396   \n",
       "3  1.233760  4.637250  5.735640  4.311743  0.078151  4.433172  4.963012   \n",
       "4  1.188722  4.624152  5.721036  4.324148  0.035837  4.437555  4.966181   \n",
       "\n",
       "          7         8         9  ...        91        92        93        94  \\\n",
       "0  1.097707  4.923816  5.728352  ...  6.758616  3.999133  4.044898  4.987543   \n",
       "1  1.105194  4.919911  5.728892  ...  6.762886  3.999778  4.040472  4.999935   \n",
       "2  1.129670  4.909443  5.748357  ...  6.765180  4.005325  4.039588  5.014343   \n",
       "3  1.207390  4.900370  5.740662  ...  6.769579  3.998335  4.026979  4.999375   \n",
       "4  1.132194  4.908939  5.740864  ...  6.764676  4.001324  4.034964  5.004535   \n",
       "\n",
       "         95        96        97        98        99  label  \n",
       "0  1.790490  4.197807  6.984423  9.260715  7.174916      1  \n",
       "1  1.792208  4.192843  6.986716  9.258625  7.172100      1  \n",
       "2  1.786922  4.206350  6.986751  9.256733  7.179493      5  \n",
       "3  1.817481  4.194461  6.987063  9.243600  7.164564      1  \n",
       "4  1.799192  4.201400  6.986776  9.252455  7.172283      1  \n",
       "\n",
       "[5 rows x 101 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we begin the classification process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GeneClassifier(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.network = nn.Sequential(\n",
    "            nn.Linear(in_features=100, out_features=16),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(in_features=16, out_features=32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(in_features=32, out_features=64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(in_features=64, out_features=128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(in_features=128, out_features=64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(in_features=64, out_features=32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(in_features=32, out_features=16),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(in_features=16, out_features=primary['label'].nunique()),\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.network(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we've defined a basic fully connected neural network, let's split our data into training and testing sets, then use K-fold CV to tune the architecture of our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.drop('label', axis=1).values\n",
    "y = [x+1 for x in df['label'].values] # So we dont have a label value of -1, noise is now label=0\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "X_train = torch.FloatTensor(X_train)\n",
    "X_test = torch.FloatTensor(X_test)\n",
    "y_train = torch.LongTensor(y_train)\n",
    "y_test = torch.LongTensor(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great, now we can define our loss function, optimization algorithmn and a model instance and get to work!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GeneClassifier(\n",
       "  (network): Sequential(\n",
       "    (0): Linear(in_features=100, out_features=16, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Linear(in_features=16, out_features=32, bias=True)\n",
       "    (3): ReLU()\n",
       "    (4): Linear(in_features=32, out_features=64, bias=True)\n",
       "    (5): ReLU()\n",
       "    (6): Linear(in_features=64, out_features=128, bias=True)\n",
       "    (7): ReLU()\n",
       "    (8): Linear(in_features=128, out_features=64, bias=True)\n",
       "    (9): ReLU()\n",
       "    (10): Linear(in_features=64, out_features=32, bias=True)\n",
       "    (11): ReLU()\n",
       "    (12): Linear(in_features=32, out_features=16, bias=True)\n",
       "    (13): ReLU()\n",
       "    (14): Linear(in_features=16, out_features=33, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = GeneClassifier()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we can train our model and view the accuracy on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 10 Loss: 2.66902232170105\n",
      "Epoch: 20 Loss: 2.525259017944336\n",
      "Epoch: 30 Loss: 2.505376100540161\n",
      "Epoch: 40 Loss: 2.492415428161621\n",
      "Epoch: 50 Loss: 2.4822139739990234\n",
      "Epoch: 60 Loss: 2.446274518966675\n",
      "Epoch: 70 Loss: 2.625608205795288\n",
      "Epoch: 80 Loss: 2.5188486576080322\n",
      "Epoch: 90 Loss: 2.4934029579162598\n",
      "Epoch: 100 Loss: 2.484018087387085\n",
      "Epoch: 110 Loss: 2.476749897003174\n",
      "Epoch: 120 Loss: 2.4636309146881104\n",
      "Epoch: 130 Loss: 2.404916763305664\n",
      "Epoch: 140 Loss: 2.65163254737854\n",
      "Epoch: 150 Loss: 2.5171375274658203\n",
      "Epoch: 160 Loss: 2.5063984394073486\n",
      "Epoch: 170 Loss: 2.4997661113739014\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/pd/jsjcl0fn7w57s5mfr34b20pm0000gn/T/ipykernel_39833/1314083038.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0my_hat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_hat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mloss_arr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/var/folders/pd/jsjcl0fn7w57s5mfr34b20pm0000gn/T/ipykernel_39833/760841815.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnetwork\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/miniconda3/envs/base-data-science/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/base-data-science/lib/python3.9/site-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    139\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    140\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 141\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    142\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    143\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/base-data-science/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/base-data-science/lib/python3.9/site-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 103\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    104\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/base-data-science/lib/python3.9/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mlinear\u001b[0;34m(input, weight, bias)\u001b[0m\n\u001b[1;32m   1846\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhas_torch_function_variadic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1847\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mhandle_torch_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1848\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_nn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1849\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1850\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "epochs = 10000\n",
    "loss_arr = []\n",
    "\n",
    "for i in range(1, epochs):\n",
    "    y_hat = model.forward(X_train)\n",
    "    loss = criterion(y_hat, y_train)\n",
    "    loss_arr.append(loss)\n",
    "    \n",
    "    if i % 10 == 0:\n",
    "        print(f'Epoch: {i} Loss: {loss}')\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig = plt.figure(figsize=(10, 5))\n",
    "plt.plot(range(1, epochs), [x.item() for x in loss_arr]);\n",
    "plt.xlabel('Epochs');\n",
    "plt.ylabel('Loss');\n",
    "plt.title('Loss vs Epochs');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor(1.2345, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.2341, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.2337, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.2333, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.2329, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.2325, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.2322, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.2318, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.2314, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.2310, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.2306, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.2302, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.2298, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.2295, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.2291, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.2287, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.2283, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.2279, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.2276, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.2272, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.2268, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.2265, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.2261, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.2257, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.2254, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.2250, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.2247, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.2243, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.2240, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.2236, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.2232, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.2229, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.2225, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.2222, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.2218, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.2215, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.2211, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.2208, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.2205, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.2201, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.2198, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.2194, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.2191, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.2187, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.2184, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.2181, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.2177, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.2174, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.2171, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.2167, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.2164, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.2161, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.2157, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.2154, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.2151, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.2147, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.2144, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.2141, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.2137, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.2134, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.2131, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.2128, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.2124, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.2121, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.2118, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.2116, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.2117, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.2129, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.2182, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.2368, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.3520, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.4513, grad_fn=<NllLossBackward0>),\n",
       " tensor(2.2373, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.8247, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.5655, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.3208, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.5692, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.8867, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.2806, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.6322, grad_fn=<NllLossBackward0>),\n",
       " tensor(2.7282, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.3088, grad_fn=<NllLossBackward0>),\n",
       " tensor(2.3441, grad_fn=<NllLossBackward0>),\n",
       " tensor(3.4088, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.6213, grad_fn=<NllLossBackward0>),\n",
       " tensor(4.0191, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.4379, grad_fn=<NllLossBackward0>),\n",
       " tensor(2.9314, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.4353, grad_fn=<NllLossBackward0>),\n",
       " tensor(2.8082, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.4806, grad_fn=<NllLossBackward0>),\n",
       " tensor(2.3039, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.3592, grad_fn=<NllLossBackward0>),\n",
       " tensor(2.0984, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.5007, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.7388, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.4722, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.5187, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.5542, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.4678, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.5102, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.3440, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.4346, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.3330, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.3746, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.3090, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.3396, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.3103, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.3168, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.2840, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.3100, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.2916, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.2941, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.2780, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.2843, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.2790, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.2735, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.2710, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.2701, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.2650, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.2656, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.2567, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.2623, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.2510, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.2606, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.2467, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.2548, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.2446, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.2491, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.2461, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.2433, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.2467, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.2388, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.2426, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.2379, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.2380, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.2383, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.2342, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.2372, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.2341, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.2328, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.2347, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.2310, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.2307, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.2313, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.2284, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.2285, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.2283, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.2263, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.2264, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.2257, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.2243, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.2243, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.2234, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.2225, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.2223, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.2216, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.2208, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.2205, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.2199, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.2191, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.2188, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.2183, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.2175, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.2171, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.2167, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.2160, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.2156, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.2151, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.2146, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.2141, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.2137, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.2132, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.2127, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.2123, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.2119, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.2114, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.2109, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.2106, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.2102, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.2097, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.2093, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.2089, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.2085, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.2081, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.2077, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.2073, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.2069, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.2065, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.2062, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.2058, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.2054, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.2051, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.2047, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.2043, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.2040, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.2036, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.2033, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.2029, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.2026, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.2023, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.2019, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.2016, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.2012, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.2009, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.2006, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.2003, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.2000, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.1996, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.1993, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.1990, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.1987, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.1984, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.1981, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.1978, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.1975, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.1972, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.1969, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.1966, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.1963, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.1960, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.1957, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.1954, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.1951, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.1948, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.1945, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.1943, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.1940, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.1937, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.1934, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.1931, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.1928, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.1925, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.1923, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.1920, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.1917, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.1914, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.1911, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.1909, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.1906, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.1903, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.1900, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.1897, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.1895, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.1892, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.1889, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.1886, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.1884, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.1881, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.1878, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.1876, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.1873, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.1871, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.1869, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.1870, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.1878, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.1901, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.1989, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.2071, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.2549, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.2665, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.3901, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.2454, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.2004, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.2009, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.2372, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.3271, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.2426, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.2147, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.1932, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.2175, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.2972, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.2713, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.3053, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.2062, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.2128, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.2798, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.2070, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.1905, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.2249, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.2306, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.2632, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.2067, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.1879, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.2081, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.2247, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.2833, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.2421, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.2494, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.1951, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.2075, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.2733, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.2305, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.2183, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.1856, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.1916, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.2184, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.2086, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.2117, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.1909, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.1818, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.1792, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.1835, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.1911, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.1901, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.1939, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.1866, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.1838, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.1784, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.1759, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.1754, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.1765, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.1800, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.1857, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.2138, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.2357, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.3927, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.3038, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.3873, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.2061, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.2562, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.5665, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.4184, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.4034, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.2027, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.4705, grad_fn=<NllLossBackward0>),\n",
       " tensor(2.1517, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.3389, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.2884, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.6737, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.2852, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.2879, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.5762, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.2427, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.2986, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.6393, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.2680, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.2609, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.4850, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.2180, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.3505, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.7383, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.2219, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.6548, grad_fn=<NllLossBackward0>),\n",
       " tensor(2.5471, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.2976, grad_fn=<NllLossBackward0>),\n",
       " tensor(2.7036, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.9073, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.9207, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.9712, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.3680, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.6935, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.2930, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.6247, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.3079, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.4774, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.4207, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.2467, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.3650, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.2264, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.3640, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.2720, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.3193, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.2806, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.2927, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.2755, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.2804, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.2561, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.2500, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.2714, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.2406, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.2868, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.2125, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.2554, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.2099, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.2400, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.2343, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.2153, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.2457, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.2034, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.2261, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.2090, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.2082, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.2181, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.1962, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.2219, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.2142, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.1986, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.2225, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.2016, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.2009, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.2136, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.1958, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.1966, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.2028, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.1897, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.1950, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.1929, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.1863, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.1912, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.1876, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.1849, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.1875, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.1844, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.1836, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.1850, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.1824, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.1818, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.1827, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.1809, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.1799, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.1804, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.1795, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.1783, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.1782, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.1782, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.1772, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.1763, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.1763, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.1760, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.1751, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.1744, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.1742, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.1739, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.1732, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.1724, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.1720, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.1718, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.1712, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.1704, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.1698, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.1693, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.1689, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.1683, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.1677, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.1671, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.1666, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.1661, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.1655, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.1649, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.1643, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.1638, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.1633, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.1628, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.1623, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.1618, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.1613, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.1608, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.1603, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.1599, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.1594, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.1590, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.1586, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.1583, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.1580, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.1577, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.1576, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.1580, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.1594, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.1645, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.1724, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.2082, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.2113, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.2713, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.2121, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.1985, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.1609, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.1671, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.2194, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.2325, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.2963, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.1984, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.1631, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.1739, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.2104, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.3048, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.2447, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.2378, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.1732, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.1961, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.3102, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.2179, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.1817, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.1562, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.1760, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.2203, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.1959, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.1945, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.1624, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.1525, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.1568, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.1678, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.1935, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.1828, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.1919, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.1644, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.1523, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.1521, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.1625, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.1941, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.2021, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.2639, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.2086, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.1909, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.1526, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.1616, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.2203, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.2382, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.3223, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.1838, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.1907, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.3243, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.2222, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.1823, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.1514, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.1735, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.2264, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.2023, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.1962, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.1572, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.1482, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.1624, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.1765, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.2173, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.1983, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.1992, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.1571, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.1492, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.1760, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.1985, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.2851, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.2291, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.2207, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.1564, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.1641, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.2688, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.2984, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.4441, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.2095, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.2321, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.4164, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.1888, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.1796, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.3426, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.2201, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.1649, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.1766, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.2062, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.2569, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.2038, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.1844, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.1526, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.1592, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.1811, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.1668, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.1565, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.1448, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.1486, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.1615, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.1592, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.1593, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.1479, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.1431, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.1410, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.1404, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.1413, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.1436, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.1500, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.1544, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.1818, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.1929, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.2940, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.2493, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.3151, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.1671, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.1698, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.3118, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.2578, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.2631, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.1648, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.1645, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.2530, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.2735, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.3418, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.1842, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.1561, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.2064, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.2111, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.2312, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.1703, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.1445, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.1465, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.1620, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.1902, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.1832, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.2103, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.1829, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.1802, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.1455, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.1392, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.1631, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.1823, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.2593, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.2303, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.2537, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.1584, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.1455, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.2271, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.2718, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.4330, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.2185, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.1803, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.2570, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.2170, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.2129, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.1544, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.1385, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.1501, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.1529, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.1511, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.1456, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.1442, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.1405, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.1423, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.1376, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.1327, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.1343, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.1421, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.1623, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.1762, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.2475, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.2256, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.2745, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.1840, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.1588, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.1347, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.1337, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.1513, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.1676, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.2254, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.2224, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.2813, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.2077, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.1873, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.1410, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.1341, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.1581, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.1785, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.2421, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.2205, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.2254, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.1583, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.1301, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.1348, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.1636, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.2671, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.2967, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.4301, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.1874, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.1974, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.4842, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.3039, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.2255, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.1498, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.2094, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.3343, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.1916, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.1376, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.1356, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.1724, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.2510, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.2179, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.1958, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.1356, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.1558, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.2494, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.2269, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.2338, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.1514, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.1559, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.2479, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.2488, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.3126, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.1762, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.1398, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.1709, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.1859, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.2297, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.1749, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.1540, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.1327, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.1483, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.1945, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.1905, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.2252, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.1763, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.1485, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.1294, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.1582, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.2474, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.2369, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.2902, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.1622, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.1710, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.3033, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.2315, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.2152, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.1410, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.1378, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.1778, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.1750, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.1956, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.1573, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.1436, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.1248, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.1287, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.1555, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.1657, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.2099, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.1789, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.1650, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.1245, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.1380, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.2239, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.2560, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.4188, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.2133, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.1494, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.2108, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.2530, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.3592, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.1912, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.1410, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.1683, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.1920, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.2379, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.1920, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.1840, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.1414, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.1289, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.1473, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.1582, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.1928, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.1657, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.1627, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.1299, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.1277, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.1660, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.1859, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.2710, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.2348, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.2552, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.1416, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.1562, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.3520, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.3006, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.3426, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.1597, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.2551, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.5153, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.2240, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.1245, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.1784, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.2138, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.2368, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.1409, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.1430, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.2423, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.2380, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.2421, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.1453, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.1425, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.2040, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.1741, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.1555, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.1188, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.1359, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.1813, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.1554, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.1434, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.1173, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.1171, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.1381, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.1448, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.1707, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.1403, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.1222, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.1131, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.1334, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.1980, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.2043, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.2884, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.1760, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.1198, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.1329, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.1940, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.3458, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.2485, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.2018, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.1324, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.1817, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.3445, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.2638, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.2236, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.1426, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.1684, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.2493, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.2119, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.2004, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.1314, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.1184, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.1411, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.1448, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.1603, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.1364, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.1294, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.1129, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.1121, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.1263, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.1332, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.1662, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.1562, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.1752, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.1315, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.1094, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.1101, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.1311, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.2024, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.2300, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.3667, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.1849, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.1156, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.1688, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.2406, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.3870, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.2180, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.1448, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.1874, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.2416, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.3354, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.2057, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.1460, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.1254, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.1500, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.1698, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.1457, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.1401, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.1187, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.1105, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.1117, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.1172, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.1346, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.1371, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.1694, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.1528, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.1478, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.1101, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.1089, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.1492, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.1801, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.3185, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.2510, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.2437, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.1225, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.1384, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.3413, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.3653, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.4842, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.1901, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.3145, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.6857, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.2706, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.1239, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.2404, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.2429, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.1837, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.1191, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.1883, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.3287, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.2154, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.1385, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.1187, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.1617, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.1934, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.1269, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.1103, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.1358, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.1331, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.1260, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.1053, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.1157, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.1633, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.1681, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.2073, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.1431, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.1092, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.1153, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.1447, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.2085, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.1667, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.1476, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.1090, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.1391, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.2644, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.2653, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.3451, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.1572, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.1738, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.3404, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.1897, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.1167, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.1189, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.1548, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.1775, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.1201, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.1010, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.1257, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.1531, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.2095, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.1665, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.1238, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.1073, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.1565, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.2656, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.2077, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.1747, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.1149, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.1614, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.3149, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.2722, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.2864, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.1426, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.1406, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.2503, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.2196, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.1961, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.1117, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.1277, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.2004, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.1652, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.1404, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.0993, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.1095, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.1573, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.1487, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.1471, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.1043, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.1007, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.1339, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.1414, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.1684, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.1272, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.1020, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.0980, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.1314, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.2295, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.2353, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.3251, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.1512, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.1324, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.2861, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.2853, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.3110, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.1441, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.1683, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.3082, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.2027, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.1349, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.0950, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.1404, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.2368, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.1817, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.1279, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.0963, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.1575, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.2996, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.2204, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.1556, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.1086, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.1768, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.3195, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.2092, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.1467, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.1054, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.1562, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.2551, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.1874, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.1463, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.0983, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.1371, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.2330, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.1793, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.1406, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.0958, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.1388, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.2486, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.1892, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.1450, grad_fn=<NllLossBackward0>),\n",
       " tensor(1.0964, grad_fn=<NllLossBackward0>)]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss_arr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to train this model much more on the full feature space with regularization methods, so for now let's try some simpler models like SVM's and tree methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "svc = SVC()\n",
    "parameters = {\n",
    "    'C': [0.1, 0.25, 0.5, 0.75, 1],\n",
    "}\n",
    "\n",
    "est = GridSearchCV(svc, parameters, verbose=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 5 candidates, totalling 25 fits\n",
      "[CV 1/5; 1/5] START C=0.1.......................................................\n"
     ]
    }
   ],
   "source": [
    "model = est.fit(X_train, y_train).best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base-data-science]",
   "language": "python",
   "name": "conda-env-base-data-science-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
